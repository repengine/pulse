"""Google Trends API â€” sentiment plugin.

Connects to Google Trends API (via pytrends) for monitoring search interest across topics.
No API key required, but uses the unofficial pytrends wrapper.
Requires pytrends package: pip install pytrends
"""

import datetime as dt
import logging
import importlib.util
import time
from typing import Dict, List, Any

from ingestion.iris_plugins import IrisPluginManager
from ingestion.iris_utils.ingestion_persistence import (
    ensure_data_directory,
    save_request_metadata,
    save_api_response,
    save_processed_data,
)

if importlib.util.find_spec("pytrends"):
    PYTRENDS_AVAILABLE = True
else:
    PYTRENDS_AVAILABLE = False
    logging.warning("pytrends package not found. Install with: pip install pytrends")

logger = logging.getLogger(__name__)

# Source name for persistence
_SOURCE_NAME = "google_trends"


class GoogleTrendsPlugin(IrisPluginManager):
    plugin_name = "google_trends_plugin"
    enabled = PYTRENDS_AVAILABLE  # Enabled if pytrends is installed
    concurrency = 1  # Limited to avoid Google throttling

    # Request configuration
    REQUEST_TIMEOUT = 30.0
    # seconds between retries (Google Trends can be sensitive to rapid requests)
    RETRY_WAIT = 5.0
    MAX_RETRIES = 3

    # Topic categories and their associated keywords
    TOPIC_CATEGORIES = {
        "finance": [
            "stock market",
            "cryptocurrency",
            "inflation",
            "recession",
            "interest rates",
            "federal reserve",
            "bitcoin",
            "tesla stock",
        ],
        "technology": [
            "artificial intelligence",
            "machine learning",
            "cybersecurity",
            "metaverse",
            "blockchain",
            "cloud computing",
            "quantum computing",
        ],
        "health": [
            "covid vaccine",
            "pandemic",
            "healthcare",
            "mental health",
            "telehealth",
            "public health",
            "virus outbreak",
        ],
        "climate": [
            "climate change",
            "renewable energy",
            "carbon emissions",
            "sustainability",
            "global warming",
            "solar power",
            "electric vehicles",
        ],
        "geopolitics": [
            "elections",
            "war",
            "sanctions",
            "trade war",
            "international relations",
            "summit",
            "military conflict",
        ],
    }

    # Geographic regions to consider
    GEO_REGIONS = {
        "global": "",  # Empty string means global
        "us": "US",  # United States
        "uk": "GB",  # United Kingdom
        "eu": "EU",  # European Union
        "in": "IN",  # India
        "cn": "CN",  # China
        "jp": "JP",  # Japan
        "br": "BR",  # Brazil
    }

    def __init__(self):
        """Initialize Google Trends plugin."""
        # Ensure data directory exists for this source
        ensure_data_directory(_SOURCE_NAME)

        if not PYTRENDS_AVAILABLE:
            self.__class__.enabled = False
            return

        try:
            # Initialize the TrendReq with English language, global region
            # Only attempt to use TrendReq if it's available
            from pytrends.request import TrendReq

            self.pytrends = TrendReq(
                hl="en-US", tz=0, timeout=(5, 25), retries=2, backoff_factor=1
            )
            self.__class__.enabled = True
        except Exception as e:
            logger.error(f"Failed to initialize pytrends: {e}")
            self.__class__.enabled = False

    def fetch_signals(self) -> List[Dict[str, Any]]:
        """Fetch search interest trends from Google Trends."""
        if not self.__class__.enabled:
            return []

        signals = []
        now = dt.datetime.now()

        # Rotate through topic categories based on the day of the month
        category_idx = now.day % len(self.TOPIC_CATEGORIES)
        category_name = list(self.TOPIC_CATEGORIES.keys())[category_idx]
        category_keywords = self.TOPIC_CATEGORIES[category_name]

        # Also rotate through regions
        region_idx = now.day % len(self.GEO_REGIONS)
        region_name = list(self.GEO_REGIONS.keys())[region_idx]
        region_code = self.GEO_REGIONS[region_name]

        # Get interest over time for up to 5 keywords in the category
        # (Google Trends limits to 5 keywords per request)
        keywords = category_keywords[:5]
        time_signals = self._get_interest_over_time(
            keywords, category_name, region_name, region_code
        )
        signals.extend(time_signals)

        # Delay to avoid rate limiting
        time.sleep(1.0)

        # Get related topics for the first keyword
        if keywords:
            related_signals = self._get_related_topics(
                keywords[0], category_name, region_name, region_code
            )
            signals.extend(related_signals)

        # Delay to avoid rate limiting
        time.sleep(1.0)

        # Get interest by region for the first keyword
        if keywords:
            geo_signals = self._get_interest_by_region(
                keywords[0], category_name, region_name, region_code
            )
            signals.extend(geo_signals)

        return signals

    def _get_interest_over_time(
        self, keywords: List[str], category: str, region_name: str, region_code: str
    ) -> List[Dict[str, Any]]:
        """Get interest over time for the given keywords."""
        signals = []

        try:
            # Build payload for the request
            timeframe = "today 3-m"  # Last 90 days
            request_params = {
                "keywords": keywords,
                "category": 0,
                "timeframe": timeframe,
                "geo": region_code,
            }

            # Save request metadata
            dataset_id = f"{category}_interest_time"
            save_request_metadata(
                dataset_id,
                request_params,
                source_name=_SOURCE_NAME,
                additional_metadata={"region_name": region_name},
            )

            self.pytrends.build_payload(
                keywords, cat=0, timeframe=timeframe, geo=region_code
            )

            # Get interest over time
            interest_over_time_df = self.pytrends.interest_over_time()

            # Save API response
            api_response = interest_over_time_df.to_dict()
            save_api_response(
                dataset_id,
                api_response,
                source_name=_SOURCE_NAME,
                timestamp=dt.datetime.now().isoformat(),
            )

            # Skip if no data
            if interest_over_time_df.empty:
                return signals

            # Get the most recent data point for each keyword
            latest_data = interest_over_time_df.iloc[-1]

            for keyword in keywords:
                if keyword in latest_data:
                    interest_value = float(latest_data[keyword])
                    normalized_keyword = keyword.replace(" ", "_").lower()

                    signal = {
                        "name": f"gtrends_{category}_{normalized_keyword}_{region_name}",
                        "value": interest_value,
                        "source": "google_trends",
                        "timestamp": dt.datetime.now(dt.timezone.utc).isoformat(),
                        "metadata": {
                            "keyword": keyword,
                            "category": category,
                            "region": region_name,
                            "relative_value": True,  # Google Trends values are relative
                        },
                    }

                    # Save processed data
                    save_processed_data(
                        f"{category}_{normalized_keyword}_{region_name}",
                        signal,
                        source_name=_SOURCE_NAME,
                        timestamp=signal["timestamp"],
                    )

                    signals.append(signal)
        except Exception as e:
            logger.warning(f"Failed to get interest over time: {e}")

        return signals

    def _get_related_topics(
        self, keyword: str, category: str, region_name: str, region_code: str
    ) -> List[Dict[str, Any]]:
        """Get related topics for a keyword."""
        signals = []

        try:
            # Build payload for the request
            timeframe = "today 3-m"  # Last 90 days
            request_params = {
                "keywords": [keyword],
                "category": 0,
                "timeframe": timeframe,
                "geo": region_code,
            }

            # Save request metadata
            dataset_id = f"{category}_{keyword.replace(' ', '_')}_related_topics"
            save_request_metadata(
                dataset_id,
                request_params,
                source_name=_SOURCE_NAME,
                additional_metadata={"region_name": region_name},
            )

            self.pytrends.build_payload(
                [keyword], cat=0, timeframe=timeframe, geo=region_code
            )

            # Get related topics
            related_topics = self.pytrends.related_topics()

            # Save API response
            save_api_response(
                dataset_id,
                {
                    "related_topics": str(related_topics)
                },  # Convert to string as it might not be JSON serializable
                source_name=_SOURCE_NAME,
                timestamp=dt.datetime.now().isoformat(),
            )

            if keyword not in related_topics:
                return signals

            # Process top related topics
            top_topics = related_topics[keyword].get("top", None)
            if top_topics is None or top_topics.empty:
                return signals

            # Get up to 5 top related topics
            for _, row in top_topics.head(5).iterrows():
                if "value" in row and "topic_title" in row:
                    topic_title = row["topic_title"]
                    interest_value = float(row["value"])
                    normalized_topic = topic_title.replace(" ", "_").lower()[
                        :20
                    ]  # Limit length

                    signal = {
                        "name": f"gtrends_{category}_related_{normalized_topic}_{region_name}",
                        "value": interest_value,
                        "source": "google_trends_related",
                        "timestamp": dt.datetime.now(
                            dt.timezone.utc).isoformat(),
                        "metadata": {
                            "keyword": keyword,
                            "related_topic": topic_title,
                            "category": category,
                            "region": region_name,
                        },
                    }

                    # Save processed data
                    save_processed_data(
                        f"{category}_related_{normalized_topic}_{region_name}",
                        signal,
                        source_name=_SOURCE_NAME,
                        timestamp=signal["timestamp"],
                    )

                    signals.append(signal)
        except Exception as e:
            logger.warning(f"Failed to get related topics: {e}")

        return signals

    def _get_interest_by_region(
        self, keyword: str, category: str, region_name: str, region_code: str
    ) -> List[Dict[str, Any]]:
        """Get interest by region for a keyword."""
        signals = []

        try:
            # Build payload for the request
            timeframe = "today 3-m"  # Last 90 days
            request_params = {
                "keywords": [keyword],
                "category": 0,
                "timeframe": timeframe,
                "geo": region_code,
            }

            # Save request metadata
            dataset_id = f"{category}_{keyword.replace(' ', '_')}_regional"
            save_request_metadata(
                dataset_id,
                request_params,
                source_name=_SOURCE_NAME,
                additional_metadata={"region_name": region_name},
            )

            self.pytrends.build_payload(
                [keyword], cat=0, timeframe=timeframe, geo=region_code
            )

            # Get interest by region
            if region_code == "":  # Global
                # Get interest by country - using interest_by_region with the proper
                # resolution
                interest_by_region_df = self.pytrends.interest_by_region(
                    resolution="COUNTRY"
                )
            else:
                # Get interest by region within country
                interest_by_region_df = self.pytrends.interest_by_region(
                    resolution="COUNTRY"
                )

            # Save API response
            api_response = interest_by_region_df.to_dict()
            save_api_response(
                dataset_id,
                api_response,
                source_name=_SOURCE_NAME,
                timestamp=dt.datetime.now().isoformat(),
            )

            # Skip if no data
            if interest_by_region_df.empty:
                return signals

            # Normalize the keyword for signal name
            normalized_keyword = keyword.replace(" ", "_").lower()

            # Calculate average interest across regions
            avg_interest = interest_by_region_df[keyword].mean()

            avg_signal = {
                "name": f"gtrends_{category}_{normalized_keyword}_avg_regional_interest",
                "value": float(avg_interest),
                "source": "google_trends_regional",
                "timestamp": dt.datetime.now(
                    dt.timezone.utc).isoformat(),
                "metadata": {
                    "keyword": keyword,
                    "category": category,
                    "region_scope": region_name,
                    "sample_size": len(interest_by_region_df),
                },
            }

            # Save processed data
            save_processed_data(
                f"{category}_{normalized_keyword}_avg_regional",
                avg_signal,
                source_name=_SOURCE_NAME,
                timestamp=avg_signal["timestamp"],
            )

            signals.append(avg_signal)

            # Calculate regional variance/dispersion
            try:
                # Using a safer approach to calculate variance
                values = interest_by_region_df[keyword].tolist()
                # Calculate variance manually to avoid complex number issues
                if len(values) > 1:
                    mean = sum(values) / len(values)
                    variance = sum((x - mean) ** 2 for x in values) / len(values)
                    interest_variance = variance
                else:
                    interest_variance = 0.0
            except (TypeError, ValueError, AttributeError) as e:
                # Handle case where variance isn't calculable
                logger.warning(
                    f"Could not calculate variance for {keyword}: {e}, using default value")
                interest_variance = 0.0

            var_signal = {
                "name": f"gtrends_{category}_{normalized_keyword}_regional_variance",
                "value": interest_variance,
                "source": "google_trends_regional",
                "timestamp": dt.datetime.now(dt.timezone.utc).isoformat(),
                "metadata": {
                    "keyword": keyword,
                    "category": category,
                    "region_scope": region_name,
                },
            }

            # Save processed data
            save_processed_data(
                f"{category}_{normalized_keyword}_regional_variance",
                var_signal,
                source_name=_SOURCE_NAME,
                timestamp=var_signal["timestamp"],
            )

            signals.append(var_signal)

        except Exception as e:
            logger.warning(f"Failed to get interest by region: {e}")

        return signals
