# Module Analysis: `iris/variable_recommender.py`

## 1. Module Intent/Purpose

The primary role of the [`iris/variable_recommender.py`](../../../iris/variable_recommender.py:1) module is to analyze variable performance logs to identify and recommend variables for integration into the PulseGrow system. It processes log data (JSONL or JSON format) containing metrics like confidence, fragility, and alignment for various variables. Based on this analysis, it recommends top-performing variables, detects drift-prone variables, and can register the recommended variables along with their performance metadata into `PulseGrow`. The module is designed to be run as a command-line interface (CLI) tool.

## 2. Operational Status/Completeness

The module appears largely functional and complete for its defined scope.
- It includes CLI argument parsing for configurability (log path, top N recommendations, minimum count, output file).
- It handles basic file I/O, including checking for log file existence.
- Logging is implemented for warnings and informational messages.
- Integration with `PulseGrow` includes a fallback mechanism: if [`PulseGrow`](../../../memory/pulsegrow.py:1) cannot be imported, the script logs a warning and continues without registration, allowing other parts (like recommendation generation and file output) to function.
- No explicit "TODO" comments or obvious major placeholders are present in the code.

## 3. Implementation Gaps / Unfinished Next Steps

- **JSON Summary File Handling:** The function [`load_variable_scores()`](../../../iris/variable_recommender.py:30) contains a comment `# Assume JSON summary` for non-JSONL files, and the [`main()`](../../../iris/variable_recommender.py:95) function has a fallback for loading non-JSONL files into `tracker.records` with the comment `# Not expected, but fallback`. This suggests that while JSONL is the primary supported format for detailed logs, the handling of pre-summarized JSON files might be less robust or was a simpler implementation path.
- **Generic Exception Handling:** The `try-except` block for importing [`PulseGrow`](../../../iris/variable_recommender.py:19-23) uses a generic `except Exception:`. More specific exception handling could improve error diagnosis.
- **Dependency on `VariablePerformanceTracker`:** The core logic for scoring and drift detection resides within [`VariablePerformanceTracker`](../../../memory/variable_performance_tracker.py:1). The completeness and accuracy of this recommender module are heavily dependent on the robustness of the tracker.
- **No Update/Removal Mechanism for PulseGrow:** The module supports registering new variables with [`PulseGrow`](../../../memory/pulsegrow.py:1) but lacks functionality to update existing variable metadata or remove variables, which could be logical extensions.
- **Drift-Prone Variable Action:** The module identifies drift-prone variables but doesn't take further action on them beyond listing them. Integration with other systems or automated responses to drift could be a future enhancement.
- **Unused Function:** The function [`load_variable_scores()`](../../../iris/variable_recommender.py:30) is defined but not directly called within the [`main()`](../../../iris/variable_recommender.py:95) execution flow. The `main` function loads data directly into an instance of `VariablePerformanceTracker`. This function might be a remnant of a previous design or intended for other uses not apparent in this script.

## 4. Connections & Dependencies

### Direct Project Module Imports:
- `from analytics.pulsegrow import PulseGrow` ([`iris/variable_recommender.py:20`](../../../iris/variable_recommender.py:20))
- `from analytics.variable_performance_tracker import VariablePerformanceTracker` ([`iris/variable_recommender.py:25`](../../../iris/variable_recommender.py:25))

### External Library Dependencies:
- `os` (Python standard library)
- `sys` (Python standard library)
- `argparse` (Python standard library)
- `logging` (Python standard library)
- `json` (Python standard library)
- `typing` (Python standard library - `List`, `Dict`, `Optional`)

### Interaction via Shared Data:
- **`PulseGrow` System:** Interacts by calling `pulse_grow.register_variable()` to add recommended variables and their metadata.
- **Variable Performance Logs:** Reads from log files (default: `logs/variable_score_log.jsonl`) which are presumably generated by other components of the Pulse system.

### Input/Output Files:
- **Input:**
    - Variable score log file: Path specified via `--log_path` CLI argument (defaults to `"logs/variable_score_log.jsonl"` ([`iris/variable_recommender.py:97`](../../../iris/variable_recommender.py:97))). Expected formats are JSONL (line-delimited JSON) or a single JSON object.
- **Output:**
    - Console output: Prints recommended variables and drift-prone variables.
    - JSON file: Optionally saves recommended and drift-prone variables to a file specified by the `--output` CLI argument ([`iris/variable_recommender.py:100`](../../../iris/variable_recommender.py:100), [`iris/variable_recommender.py:125-126`](../../../iris/variable_recommender.py:125-126)).

## 5. Function and Class Example Usages

- **`load_variable_scores(log_path: str) -> Dict[str, Dict]`** ([`iris/variable_recommender.py:30`](../../../iris/variable_recommender.py:30))
    - **Description:** Loads variable performance data from a specified log file (JSONL or JSON). It aggregates metrics such as `count`, `confidence`, `fragility`, `certified`, and `alignment_score` for each variable and computes averages.
    - **Example (Conceptual):** `scores_data = load_variable_scores("path/to/variable_scores.jsonl")`
    *(Note: This function is not directly used in the `main()` flow of the script.)*

- **`recommend_variables_by_impact(tracker: VariablePerformanceTracker, top_n: int = 10, min_count: int = 5) -> List[Dict]`** ([`iris/variable_recommender.py:65`](../../../iris/variable_recommender.py:65))
    - **Description:** Takes an instance of `VariablePerformanceTracker`, retrieves scored variables, ranks them primarily by `avg_confidence` (descending), filters out variables with a `count` less than `min_count`, and returns a list of the top `top_n` recommended variables with their statistics.
    - **Example (within `main`):** `recommended = recommend_variables_by_impact(tracker, top_n=args.top_n, min_count=args.min_count)`

- **`detect_drift_prone(tracker: VariablePerformanceTracker, threshold: float = 0.25) -> List[str]`** ([`iris/variable_recommender.py:76`](../../../iris/variable_recommender.py:76))
    - **Description:** Utilizes the `VariablePerformanceTracker` instance to identify variables whose drift metric (as calculated by the tracker) exceeds a specified `threshold`. Returns a list of names of these drift-prone variables.
    - **Example (within `main`):** `drift_prone = detect_drift_prone(tracker)` (uses default threshold from tracker or one set on tracker)

- **`register_variables_with_metadata(variables: List[Dict])`** ([`iris/variable_recommender.py:79`](../../../iris/variable_recommender.py:79))
    - **Description:** Takes a list of recommended variable dictionaries. If `PulseGrow` is available, it iterates through this list, extracts relevant metadata (name, avg_confidence, avg_fragility, etc.), and calls `pulse_grow.register_variable()` for each variable.
    - **Example (within `main`):** `register_variables_with_metadata(recommended)`

- **`main()`** ([`iris/variable_recommender.py:95`](../../../iris/variable_recommender.py:95))
    - **Description:** The main execution function when the script is run. It handles:
        1. Parsing CLI arguments.
        2. Initializing `VariablePerformanceTracker` and loading records from the log file.
        3. Calling `recommend_variables_by_impact` and `detect_drift_prone`.
        4. Printing results to the console.
        5. Optionally writing results to an output JSON file.
        6. Calling `register_variables_with_metadata` to register variables with `PulseGrow`.
    - **Example Usage (CLI):**
      ```bash
      python -m ingestion.variable_recommender --log_path logs/variable_score_log.jsonl --top_n 10 --min_count 5 --output recommended_output.json
      ```

## 6. Hardcoding Issues

- **Default Log Path:** The default path for the variable score log is hardcoded as `"logs/variable_score_log.jsonl"` within the `argparse` definition ([`iris/variable_recommender.py:97`](../../../iris/variable_recommender.py:97)). While configurable via CLI, this default is embedded.
- **Logger Name:** The logger is named `"variable_recommender"` ([`iris/variable_recommender.py:27`](../../../iris/variable_recommender.py:27)).
- **Drift Threshold Default:** The `detect_drift_prone` function has a default `threshold` of `0.25` ([`iris/variable_recommender.py:76`](../../../iris/variable_recommender.py:76)).
- **Metadata Keys for Registration:** The keys used to construct the `metadata` dictionary in [`register_variables_with_metadata()`](../../../iris/variable_recommender.py:84-91) (e.g., `"name"`, `"avg_confidence"`, `"avg_fragility"`, `"certified_ratio"`, `"avg_alignment"`, `"count"`) are hardcoded strings. These are dependent on the output structure of `VariablePerformanceTracker` and the expected input for `PulseGrow`.
- **Log Record Keys:** Keys used when parsing log records in [`load_variable_scores()`](../../../iris/variable_recommender.py:44-53) (e.g., `"variable"`, `"confidence"`, `"fragility"`, `"alignment_score"`, `"certified"`) are hardcoded. These are tied to the specific format of the input log files. The `main` function relies on `VariablePerformanceTracker` to handle these keys internally when loading records.

## 7. Coupling Points

- **`analytics.pulsegrow.PulseGrow`:** The module is tightly coupled with `PulseGrow` for its registration feature. If `PulseGrow`'s API changes (e.g., `register_variable` method signature or metadata expectations), this module would require updates. The fallback mechanism mitigates complete failure if `PulseGrow` is absent.
- **`analytics.variable_performance_tracker.VariablePerformanceTracker`:** This is a significant coupling point. The recommender module delegates the core tasks of data loading from logs, variable scoring, and drift detection to `VariablePerformanceTracker`. Changes in the tracker's methods (e.g., `score_variable_effectiveness()`, `detect_variable_drift()`) or the structure of data it returns would directly impact this module.
- **Log File Format:** The module's ability to process variable performance data (either directly via `load_variable_scores` or indirectly via `VariablePerformanceTracker`) is dependent on the structure and keys present in the input JSONL/JSON log files.
- **Output File Format:** The structure of the JSON output file (if generated) is defined within the [`main()`](../../../iris/variable_recommender.py:126) function. Consumers of this file would depend on this structure.

## 8. Existing Tests

- Based on the provided file list and the module's nature as a script, there is no immediately apparent dedicated test file (e.g., `tests/iris/test_variable_recommender.py`).
- The module is executable via `if __name__ == "__main__":`.
- **Assessment:** Formal unit tests for this module are likely missing. Testing would require:
    - Mocking input log files with various scenarios (JSONL, JSON, empty, malformed).
    - Mocking `VariablePerformanceTracker` to control its output and verify interactions.
    - Mocking `PulseGrow` to verify calls to `register_variable` and the metadata passed.
    - Testing CLI argument parsing and handling.
    - Verifying console output and the structure/content of the output JSON file.
- The core logic for scoring and drift is within `VariablePerformanceTracker`, which should have its own comprehensive tests.

## 9. Module Architecture and Flow

1.  **Initialization:**
    *   Standard library imports (`os`, `sys`, `argparse`, `logging`, `json`, `typing`).
    *   Project-specific imports: `PulseGrow` (with a fallback if import fails) and `VariablePerformanceTracker`.
    *   Logger setup.
2.  **CLI Entry (`if __name__ == "__main__":` -> `main()`):**
    *   An `ArgumentParser` is configured to accept `--log_path`, `--top_n`, `--min_count`, and `--output`.
    *   An instance of `VariablePerformanceTracker` is created.
    *   The script attempts to load records from the `args.log_path` into the `tracker.records`. If the log file doesn't exist, it logs a warning and exits.
    *   `recommend_variables_by_impact()` is called with the tracker and CLI arguments (`top_n`, `min_count`) to get a list of recommended variables.
    *   `detect_drift_prone()` is called with the tracker to get a list of drift-prone variable names.
    *   The recommended and drift-prone variables are printed to the standard output.
    *   If `args.output` is specified, the results (recommended and drift-prone lists) are written to a JSON file.
    *   `register_variables_with_metadata()` is called with the list of recommended variables to register them with `PulseGrow` (if `pulse_grow` instance is available).
3.  **Core Functions:**
    *   **`load_variable_scores()`:** (Currently unused by `main`) Reads a log file, aggregates scores, and computes averages.
    *   **`recommend_variables_by_impact()`:** Filters and ranks variables based on scores from `VariablePerformanceTracker`.
    *   **`detect_drift_prone()`:** Uses `VariablePerformanceTracker` to identify variables exceeding a drift threshold.
    *   **`register_variables_with_metadata()`:** Prepares metadata and calls `pulse_grow.register_variable()` for each recommended variable.

## 10. Naming Conventions

- **Functions and Variables:** Generally follow Python's `snake_case` convention (e.g., [`load_variable_scores`](../../../iris/variable_recommender.py:30), [`avg_confidence`](../../../iris/variable_recommender.py:59)). This aligns with PEP 8.
- **Class Names (Imported):** [`PulseGrow`](../../../memory/pulsegrow.py:1) and [`VariablePerformanceTracker`](../../../memory/variable_performance_tracker.py:1) use `PascalCase`, which is standard for Python classes.
- **Logger Instance:** `logger` is lowercase, which is common.
- **Constants:** No global constants are defined in a way that would require `UPPER_SNAKE_CASE`.
- **Consistency:** Naming is consistent throughout the module.
- **Clarity:** Names are generally descriptive and clearly indicate the purpose of functions and variables (e.g., `recommend_variables_by_impact`, `min_count`).
- **AI Assumption Errors:** No obvious errors in naming conventions that would suggest misinterpretation by AI or significant deviation from common Python practices.