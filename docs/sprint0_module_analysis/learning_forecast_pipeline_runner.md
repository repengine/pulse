# Analysis Report: `learning/forecast_pipeline_runner.py`

## 1. Module Intent/Purpose

The primary role of the [`learning/forecast_pipeline_runner.py`](learning/forecast_pipeline_runner.py:1) module is to orchestrate the full processing pipeline for forecasts generated by the Pulse system. This includes several stages:
- Initial scoring of forecasts based on trust and fragility.
- Filtering forecasts based on confidence levels.
- Detecting contradictions between forecasts.
- Optionally applying symbolic overlays to forecasts.
- Assigning trace metadata and registering forecasts to memory.
- Integrating with an "Epistemic Mirror" (GPT-based components) for rule extraction, fingerprinting, convergence loss calculation, and divergence logging.
- Compressing and summarizing forecasts.
- Generating a "Strategos Digest" from the processed forecasts.
- Storing processed forecasts into a persistent `ForecastMemory`.
- Selecting top forecasts for strategic output and auto-promoting them.
- Logging variable performance and recommending new variables.
- Interacting with a "Capital Layer" for exposure and alignment summaries (though this part seems less developed or dependent on external state).

## 2. Operational Status/Completeness

The module appears to be largely complete and operational for its defined scope. It handles various stages of forecast processing and includes error handling for several steps.

- **Placeholders/TODOs**:
    - Line [`154`](learning/forecast_pipeline_runner.py:154): `input_state={} # Replace with state.to_dict() if state is available`
    - Line [`157`](learning/forecast_pipeline_runner.py:157): `input_state = f.get("input_state", {}) # Replace with actual sim state if available`
    - Lines [`301-303`](learning/forecast_pipeline_runner.py:301-303) regarding Capital Layer integration: `# If you have a WorldState object, pass it here. Otherwise, adapt as needed.` This suggests that the `WorldState` object, crucial for these functions, is not directly available or passed into this pipeline, making this section conditional or incomplete.

## 3. Implementation Gaps / Unfinished Next Steps

- **Capital Layer Integration**: As noted above, the integration with the `capital_engine` ([`capital_engine/capital_layer.py`](capital_engine/capital_layer.py:1)) seems to be a significant area that is either partially implemented or requires external state (`WorldState`) that isn't consistently available within this pipeline's context. The comments explicitly state this dependency.
- **State Management for Epistemic Mirror**: While the Epistemic Mirror components are called, the way `pulse_data` and `gpt_data` are retrieved (lines [`186-187`](learning/forecast_pipeline_runner.py:186-187)) using `f.get("pulse_data")` and `f.get("gpt_data")` suggests that these complex, structured forecast objects (expected to conform to `ForecastSchema`) might not always be present or correctly populated in the main `forecast` dictionary `f`. The pipeline attempts to proceed even if these are `None`.
- **Error Handling in Epistemic Mirror**: The `try-except` block around `compute_symbolic_convergence_loss` (lines [`194-202`](learning/forecast_pipeline_runner.py:194-202)) and `tag_divergence_type` (lines [`205-213`](learning/forecast_pipeline_runner.py:205-213)) logs errors but allows the pipeline to continue. Depending on the criticality of these steps, more sophisticated error handling or conditional logic might be needed.
- **Variable Recommender Integration**: The variable recommender is run as a subprocess (lines [`278-289`](learning/forecast_pipeline_runner.py:278-289)). This is a somewhat loose coupling. A more integrated approach might be considered for better error handling and data flow. The fixed path `logs/recommended_vars.json` is also a point of potential inflexibility.

## 4. Connections & Dependencies

**Direct Imports from Other Project Modules:**

*   [`core.pulse_config`](core/pulse_config.py:1): `USE_SYMBOLIC_OVERLAYS`
*   [`forecast_output.forecast_compressor`](forecast_output/forecast_compressor.py:1): [`compress_forecasts()`](forecast_output/forecast_compressor.py:1)
*   [`intelligence.forecast_schema`](intelligence/forecast_schema.py:1): `ForecastSchema`
*   [`forecast_output.forecast_summary_synthesizer`](forecast_output/forecast_summary_synthesizer.py:1): [`summarize_forecasts()`](forecast_output/forecast_summary_synthesizer.py:1)
*   [`forecast_output.strategos_digest_builder`](forecast_output/strategos_digest_builder.py:1): [`build_digest()`](forecast_output/strategos_digest_builder.py:1)
*   [`trust_system.trust_engine`](trust_system/trust_engine.py:1): `TrustEngine`
*   [`trust_system.fragility_detector`](trust_system/fragility_detector.py:1): [`tag_fragility()`](trust_system/fragility_detector.py:1)
*   [`analytics.trace_audit_engine`](memory/trace_audit_engine.py:1): [`assign_trace_metadata()`](memory/trace_audit_engine.py:1), [`register_trace_to_memory()`](memory/trace_audit_engine.py:1)
*   [`analytics.forecast_memory`](memory/forecast_memory.py:1): `ForecastMemory`
*   [`utils.log_utils`](utils/log_utils.py:1): [`log_info()`](utils/log_utils.py:1)
*   [`forecast_output.forecast_prioritization_engine`](forecast_output/forecast_prioritization_engine.py:1): [`select_top_forecasts()`](forecast_output/forecast_prioritization_engine.py:1)
*   [`analytics.forecast_memory_promoter`](memory/forecast_memory_promoter.py:1): [`select_promotable_forecasts()`](memory/forecast_memory_promoter.py:1), [`export_promoted()`](memory/forecast_memory_promoter.py:1)
*   [`capital_engine.capital_layer`](capital_engine/capital_layer.py:1): [`run_capital_forks()`](capital_engine/capital_layer.py:1), [`summarize_exposure()`](capital_engine/capital_layer.py:1), [`portfolio_alignment_tags()`](capital_engine/capital_layer.py:1) (Note: `run_capital_forks` is imported but not directly used in `run_forecast_pipeline`)
*   [`forecast_output.forecast_confidence_gate`](forecast_output/forecast_confidence_gate.py:1): [`filter_by_confidence()`](forecast_output/forecast_confidence_gate.py:1)
*   [`analytics.trace_memory`](memory/trace_memory.py:1): `TraceMemory`
*   [`analytics.variable_performance_tracker`](memory/variable_performance_tracker.py:1): `VariablePerformanceTracker`
*   [`forecast_output.forecast_contradiction_detector`](forecast_output/forecast_contradiction_detector.py:1): [`detect_forecast_contradictions()`](forecast_output/forecast_contradiction_detector.py:1)
*   [`core.pulse_learning_log`](core/pulse_learning_log.py:1): [`log_learning_event()`](core/pulse_learning_log.py:1)
*   [`GPT.gpt_causal_translator`](GPT/gpt_causal_translator.py:1): [`extract_rules_from_gpt_output()`](GPT/gpt_causal_translator.py:1), [`label_symbolic_arcs()`](GPT/gpt_causal_translator.py:1), [`identify_missing_domains()`](GPT/gpt_causal_translator.py:1)
*   [`GPT.gpt_rule_fingerprint_extractor`](GPT/gpt_rule_fingerprint_extractor.py:1): [`extract_fingerprint_from_gpt_rationale()`](GPT/gpt_rule_fingerprint_extractor.py:1), [`match_fingerprint_to_pulse_rules()`](GPT/gpt_rule_fingerprint_extractor.py:1), [`archive_foreign_fingerprint()`](GPT/gpt_rule_fingerprint_extractor.py:1)
*   [`GPT.gpt_symbolic_convergence_loss`](GPT/gpt_symbolic_convergence_loss.py:1): [`compute_symbolic_convergence_loss()`](GPT/gpt_symbolic_convergence_loss.py:1)
*   [`GPT.gpt_forecast_divergence_logger`](GPT/gpt_forecast_divergence_logger.py:1): [`tag_divergence_type()`](GPT/gpt_forecast_divergence_logger.py:1), [`log_forecast_divergence()`](GPT/gpt_forecast_divergence_logger.py:1)
*   [`symbolic_system.pulse_symbolic_arc_tracker`](symbolic_system/pulse_symbolic_arc_tracker.py:1): [`compute_arc_label()`](symbolic_system/pulse_symbolic_arc_tracker.py:1) (Dynamically imported if `USE_SYMBOLIC_OVERLAYS` is true)

**External Library Dependencies:**

*   `typing`: `List`, `Dict`, `Optional`, `Any`
*   `pydantic`: `ValidationError` (explicitly imported), `BaseModel` (implicitly used via `ForecastSchema`)
*   `os`: For path manipulation (`os.path.join`, `os.path.dirname`, `os.makedirs`)
*   `json`: For `json.dumps`
*   `subprocess`: For running the variable recommender script.

**Interaction with Other Modules via Shared Data:**

*   **Forecast Memory**: Reads from and writes to `ForecastMemory` ([`memory/forecast_memory.py`](memory/forecast_memory.py:1)).
*   **Trace Memory**: Writes to `TraceMemory` ([`memory/trace_memory.py`](memory/trace_memory.py:1)).
*   **Learning Log**: Writes events using [`core/pulse_learning_log.py`](core/pulse_learning_log.py:1).
*   **Variable Recommender**: Interacts via a subprocess call and potentially a JSON file ([`logs/recommended_vars.json`](logs/recommended_vars.json)).
*   **GPT Fingerprint Archive**: The [`archive_foreign_fingerprint()`](GPT/gpt_rule_fingerprint_extractor.py:1) function likely writes to a persistent store, though its details are not in this module.
*   **Configuration**: Reads `USE_SYMBOLIC_OVERLAYS` from [`core/pulse_config.py`](core/pulse_config.py:1).

**Input/Output Files:**

*   **Input**: Expects a list of forecast dictionaries. The structure of these dictionaries is partially implied by usage (e.g., must have 'confidence', 'symbolic_tag', and potentially 'gpt_output', 'pulse_data', 'gpt_data', 'trace_id', etc.).
*   **Output (Logs/Data Files)**:
    *   `logs/strategic_batch_output.jsonl`: Stores top N selected forecasts.
    *   `logs/recommended_vars.json`: Output from the variable recommender script.
    *   General logging via [`utils/log_utils.log_info()`](utils/log_utils.py:1).
    *   Trace memory logs (managed by `TraceMemory`).
    *   Forecast memory storage (managed by `ForecastMemory`).
    *   Variable performance scores export (managed by `VariablePerformanceTracker.export_variable_scores()`, path not specified here).

## 5. Function and Class Example Usages

*   **[`run_forecast_pipeline(forecasts, batch_id=None, enable_digest=True, save_to_memory=True)`](learning/forecast_pipeline_runner.py:54)**:
    *   This is the main function of the module.
    *   **Usage**: It takes a list of raw forecast dictionaries. Each dictionary should contain forecast data.
    *   It processes these forecasts through various stages like trust scoring, fragility tagging, confidence filtering, symbolic overlay application, trace assignment, Epistemic Mirror integration (rule extraction, fingerprinting, convergence loss, divergence logging), compression, summarization, digest building, and memory storage.
    *   It returns a dictionary (`result_bundle`) containing the status of the pipeline, counts of total and compressed forecasts, the generated digest, and the top selected forecasts.
    *   Example from `_test_pipeline()`:
        ```python
        sample = [
            {"confidence": 0.71, "symbolic_tag": "hope", "drivers": ["AI rally"]},
            {"confidence": 0.43, "symbolic_tag": "fatigue", "drivers": ["media overload"]}
        ]
        result = run_forecast_pipeline(sample)
        ```

*   **`ForecastSchema` (from [`intelligence/forecast_schema.py`](intelligence/forecast_schema.py:1))**:
    *   Used implicitly for validating `pulse_data` and `gpt_data` before computing symbolic convergence loss (lines [`196-197`](learning/forecast_pipeline_runner.py:196-197)).
    *   Pydantic's `BaseModel` is the foundation for `ForecastSchema`. It ensures that the forecast data adheres to a defined structure and types. If validation fails, a `ValidationError` is caught.

*   **`ValidationError` (from `pydantic`)**:
    *   Caught during the validation of `pulse_data` and `gpt_data` against `ForecastSchema` (line [`200`](learning/forecast_pipeline_runner.py:200)). This indicates that Pydantic is used for data validation, and errors during this process are handled.

## 6. Hardcoding Issues

*   **File Paths**:
    *   `"logs/strategic_batch_output.jsonl"` (line [`251`](learning/forecast_pipeline_runner.py:251))
    *   `"--output", "logs/recommended_vars.json"` for the variable recommender script (line [`284`](learning/forecast_pipeline_runner.py:284)).
    These paths are hardcoded, which might reduce flexibility if the logging directory structure needs to change. Using a centralized path management system (like `PathRegistry` if available and suitable) would be better.
*   **Magic Numbers/Strings**:
    *   `top_n=5` for [`select_top_forecasts()`](forecast_output/forecast_prioritization_engine.py:1) (line [`250`](learning/forecast_pipeline_runner.py:250)).
    *   Variable recommender script parameters: `"--top_n", "10"`, `"--min_count", "5"` (lines [`282-283`](learning/forecast_pipeline_runner.py:282-283)).
    *   Default string "arc_unknown" (line [`140`](learning/forecast_pipeline_runner.py:140), [`142`](learning/forecast_pipeline_runner.py:142)).
    *   Status strings like `"no_forecasts"`, `"error"`, `"complete"`, `"❌ Rejected"`, `"❌ Contradictory"`. While common, centralizing these as constants could improve maintainability.
    *   Keys used for dictionary access like `"confidence"`, `"symbolic_tag"`, `"trace_id"`, `"gpt_output"`, `"pulse_data"`, etc. These are fundamental to the forecast data structure but are used as string literals throughout. Defining these as constants or part of a schema definition (which `ForecastSchema` likely does for its own fields) would be more robust.

## 7. Coupling Points

*   **High Coupling with Specific Module Implementations**: The pipeline directly calls functions from many different modules. While this is necessary for an orchestrator, changes in the function signatures or core logic of these dependent modules will directly impact this pipeline.
*   **Data Structure Dependency**: The pipeline heavily relies on the specific dictionary structure of the `forecasts` list and the data added/modified by each processing step. Changes to this implicit schema can break subsequent steps. The use of `ForecastSchema` for `pulse_data` and `gpt_data` is a good step towards mitigating this for those specific parts, but the overall forecast object `f` is manipulated as a general dictionary.
*   **`core.pulse_config.USE_SYMBOLIC_OVERLAYS`**: This global config flag directly controls a branch of logic within the pipeline.
*   **Error Handling Strategy**: The current strategy is mostly to log errors and continue, or return an error status. This might be acceptable, but for critical steps, a more nuanced error propagation or handling mechanism might be needed.
*   **Subprocess Call**: The call to `irldata.variable_recommender` is a form of coupling. Failures in this external script are caught, but the interaction is less controlled than direct Python calls.
*   **Implicit State in Forecast Dictionaries**: Many steps add keys to the forecast dictionaries (e.g., `confidence_status`, `arc_label`, `gpt_extracted_rules`). Subsequent steps rely on these keys being present. This creates an implicit dependency on the order of operations and the successful completion of prior steps.

## 8. Existing Tests

*   A basic test function `_test_pipeline()` exists within the module (lines [`307-319`](learning/forecast_pipeline_runner.py:307-319)).
    *   It uses a very small, simple sample of two forecasts.
    *   It primarily checks if the pipeline runs without crashing and prints the JSON output.
    *   It does not perform assertions on the content or correctness of the output beyond basic execution.
*   **Coverage**: This single test provides minimal coverage. It doesn't test:
    *   Edge cases (e.g., empty forecast list, malformed forecasts).
    *   The behavior of each individual pipeline stage in detail.
    *   The Epistemic Mirror integration.
    *   The Capital Layer integration.
    *   Error conditions and recovery.
    *   The effect of `USE_SYMBOLIC_OVERLAYS` being True/False.
*   **External Test File**: There is no direct indication of a corresponding `tests/learning/test_forecast_pipeline_runner.py`. A search for such a file would be needed to confirm if more comprehensive tests exist elsewhere. Assuming none for now based on the provided context.
*   **Gaps**: Significant testing gaps exist. More comprehensive unit and integration tests are needed to ensure the reliability of this critical pipeline.

## 9. Module Architecture and Flow

**High-Level Structure:**
The module is centered around the `run_forecast_pipeline` function, which acts as an orchestrator. It takes a list of raw forecasts and processes them sequentially through various stages.

**Key Components/Stages:**
1.  **Input Validation**: Basic check for forecast list presence and type.
2.  **Trust & Fragility Scoring**: Uses `TrustEngine` and `tag_fragility`. Filters by confidence.
3.  **Contradiction Detection**: Uses `detect_forecast_contradictions` and logs events.
4.  **Confidence Gating**: Applies `filter_by_confidence` again.
5.  **Symbolic Overlays (Conditional)**: If `USE_SYMBOLIC_OVERLAYS` is true, attempts to compute and add `arc_label`.
6.  **Trace Management**: Assigns trace metadata, registers to memory, logs to `TraceMemory`, and logs to `VariablePerformanceTracker`.
7.  **Epistemic Mirror Integration**:
    *   Extracts rules, arcs, missing domains from GPT output.
    *   Extracts, matches, or archives GPT causal fingerprints.
    *   Computes symbolic convergence loss between Pulse and GPT forecasts (if data conforms to `ForecastSchema`).
    *   Tags and logs forecast divergence.
8.  **Forecast Processing**:
    *   Compression (`compress_forecasts`).
    *   Symbolic summarization (`summarize_forecasts`).
9.  **Digest Generation (Conditional)**: Uses `build_digest`.
10. **Memory Storage (Conditional)**: Stores compressed forecasts in `ForecastMemory`.
11. **Top Forecast Selection & Promotion**: Selects top N, exports them, and promotes some to memory.
12. **Variable Performance Export**: Exports scores from `VariablePerformanceTracker`.
13. **Variable Recommendation**: Runs an external Python script via `subprocess`.
14. **Output**: Returns a result bundle.

**Primary Data/Control Flow:**
- Data (forecasts) flows sequentially through the stages. Each stage typically modifies the forecast objects (which are dictionaries) by adding new keys or updating existing ones.
- Control flow is largely linear, with conditional branches for `USE_SYMBOLIC_OVERLAYS`, `enable_digest`, and `save_to_memory`.
- Error handling is present at several key stages, usually involving logging the error and either continuing with potentially partial data or returning an error status.

## 10. Naming Conventions

*   **Functions**: Generally follow PEP 8 (snake_case), e.g., [`run_forecast_pipeline()`](learning/forecast_pipeline_runner.py:54), [`assign_trace_metadata()`](memory/trace_audit_engine.py:1). The internal test function `_test_pipeline()` uses a leading underscore, which is a common convention for internal/utility functions.
*   **Classes**: Imported classes use PascalCase (e.g., `TrustEngine`, `ForecastMemory`, `ForecastSchema`, `VariablePerformanceTracker`).
*   **Variables**:
    *   Local variables are mostly snake_case (e.g., `batch_id`, `enable_digest`, `scored`, `compressed`, `digest_result`).
    *   Some single-letter variables are used in loops (`f` for forecast, `e` for exception), which is acceptable for small scopes.
    *   `USE_SYMBOLIC_OVERLAYS` is an imported constant in UPPER_SNAKE_CASE, following convention.
*   **Module Name**: `forecast_pipeline_runner.py` is snake_case, which is standard.
*   **Consistency**: Naming seems largely consistent with Python conventions (PEP 8).
*   **AI Assumption Errors/Deviations**:
    *   The variable `div_type` (line [`209`](learning/forecast_pipeline_runner.py:209)) has a typo: `f["diverggence_type"] = div_type`. It should likely be `divergence_type`. This is a minor but noticeable deviation.
    *   The author tag "Pulse v0.23" (line [`14`](learning/forecast_pipeline_runner.py:14)) might be an AI-generated placeholder or an artifact of an automated system, rather than a human author.

Overall, naming conventions are good, with minor exceptions.