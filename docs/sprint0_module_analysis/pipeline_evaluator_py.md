# Module Analysis: `pipeline/evaluator.py`

## 1. Module Intent/Purpose

The primary role of the [`pipeline/evaluator.py`](pipeline/evaluator.py:1) module is to run model evaluation benchmarks. It compares forecasts generated by the Pulse system against forecasts generated by a GPT model (assumed to be ground truth or a more accurate baseline). Based on this comparison, it calculates divergence metrics (e.g., symbolic convergence loss) and proposes rule changes for the Pulse system to improve its forecasting accuracy and alignment with the GPT model's outputs.

## 2. Operational Status/Completeness

The module appears to be partially complete but operational for its core task of comparing forecasts and proposing rule changes based on divergence.

*   **Placeholders/TODOs:**
    *   [`__init__`](pipeline/evaluator.py:16): `TODO: set up simulation_client or other services`
    *   [`evaluate`](pipeline/evaluator.py:44): `TODO: Replace with actual data loading mechanism if different` (currently assumes Parquet)
    *   [`evaluate`](pipeline/evaluator.py:86): `TODO: Implement more sophisticated feedback generation and storage`
    *   [`evaluate`](pipeline/evaluator.py:88): `TODO: Integrate proposed_rule_changes into a management mechanism`
    *   [`evaluate`](pipeline/evaluator.py:110): `TODO: implement evaluation logic calling engine.simulate_forward`
    *   [`evaluate`](pipeline/evaluator.py:111): `TODO: Incorporate historical forecast data for evaluation and training`
    *   [`analyze_and_propose_rule_changes`](pipeline/evaluator.py:209): `TODO: Implement more sophisticated rule generation/pruning logic`

## 3. Implementation Gaps / Unfinished Next Steps

*   **More Extensive Intentions:**
    *   The TODO in [`__init__`](pipeline/evaluator.py:16) suggests an intention to interact with a simulation client, which is not currently implemented.
    *   The TODO in [`evaluate`](pipeline/evaluator.py:110) to call `engine.simulate_forward` indicates a planned but missing step for a more comprehensive evaluation process, possibly involving running simulations with proposed changes.
    *   The TODO in [`evaluate`](pipeline/evaluator.py:111) about incorporating historical forecast data suggests a broader scope for evaluation and training feedback loops.
*   **Logical Next Steps:**
    *   A mechanism to apply or manage the [`proposed_rule_changes.json`](pipeline/rule_proposals/proposed_rule_changes.json:1) (as noted in [`evaluate`](pipeline/evaluator.py:88)).
    *   More sophisticated data loading beyond just a Parquet file path ([`evaluate`](pipeline/evaluator.py:44)).
    *   Advanced feedback storage and management ([`evaluate`](pipeline/evaluator.py:86)).
*   **Deviations/Stoppages:**
    *   The rule proposal logic in [`analyze_and_propose_rule_changes`](pipeline/evaluator.py:120) is described as basic, with a TODO for more sophistication ([`analyze_and_propose_rule_changes`](pipeline/evaluator.py:209)). This suggests development may have paused before implementing more advanced rule adaptation techniques.

## 4. Connections & Dependencies

*   **Direct Project Imports:**
    *   `from GPT.gpt_symbolic_convergence_loss import compute_symbolic_convergence_loss, decompose_loss_components` ([`pipeline/evaluator.py:8`](pipeline/evaluator.py:8))
    *   `from intelligence.forecast_schema import ForecastSchema` ([`pipeline/evaluator.py:9`](pipeline/evaluator.py:9))
*   **External Library Dependencies:**
    *   `typing` (Dict, Any, List) ([`pipeline/evaluator.py:7`](pipeline/evaluator.py:7))
    *   `pydantic` (ValidationError) ([`pipeline/evaluator.py:10`](pipeline/evaluator.py:10))
    *   `pandas` (as pd) ([`pipeline/evaluator.py:11`](pipeline/evaluator.py:11))
    *   `json` ([`pipeline/evaluator.py:12`](pipeline/evaluator.py:12))
    *   `os` ([`pipeline/evaluator.py:13`](pipeline/evaluator.py:13))
*   **Interaction via Shared Data:**
    *   **Input:** Reads forecast data from a Parquet file specified by `feature_path` ([`evaluate`](pipeline/evaluator.py:45)). This file is expected to contain 'pulse_output' and 'gpt_struct' columns.
    *   **Output:** Writes proposed rule changes to a JSON file: [`pipeline/rule_proposals/proposed_rule_changes.json`](pipeline/rule_proposals/proposed_rule_changes.json:1) ([`evaluate`](pipeline/evaluator.py:104-106)).
*   **Input/Output Files:**
    *   **Input:** `feature_path` (e.g., a `.parquet` file containing forecast episodes).
    *   **Output:** [`pipeline/rule_proposals/proposed_rule_changes.json`](pipeline/rule_proposals/proposed_rule_changes.json:1).

## 5. Function and Class Example Usages

*   **Class: `Evaluator`**
    *   **Initialization:**
        ```python
        evaluator = Evaluator()
        ```
    *   **Evaluation:**
        ```python
        model_info = {"name": "PulseModel_v1", "version": "1.0"}
        feature_data_path = "path/to/forecast_episodes.parquet"
        metrics = evaluator.evaluate(model_info, feature_data_path)
        print(metrics)
        # This would also generate/update 'pipeline/rule_proposals/proposed_rule_changes.json'
        ```
*   **Method: `evaluate(model_info: Dict, feature_path: str) -> Dict`**
    *   Loads forecast data, iterates through episodes, computes symbolic convergence loss between `pulse_output` and `gpt_struct` using [`compute_symbolic_convergence_loss`](GPT/gpt_symbolic_convergence_loss.py:1) and [`decompose_loss_components`](GPT/gpt_symbolic_convergence_loss.py:1), then calls [`analyze_and_propose_rule_changes`](pipeline/evaluator.py:120) to generate feedback, and saves these proposed changes.
*   **Method: `analyze_and_propose_rule_changes(pulse_forecast: ForecastSchema, gpt_forecast: ForecastSchema, loss_components: Dict) -> List[Dict]`**
    *   Takes Pulse and GPT forecast objects (as [`ForecastSchema`](intelligence/forecast_schema.py:1)) and loss components. It then compares actions, state, and rationale to propose rule changes like "add_rule", "prune_rule", or "modify_rule".

## 6. Hardcoding Issues

*   **Output Directory/File:** The path for saving proposed rule changes is hardcoded: `output_dir = "pipeline/rule_proposals"` ([`evaluate`](pipeline/evaluator.py:102)) and `output_path = os.path.join(output_dir, "proposed_rule_changes.json")` ([`evaluate`](pipeline/evaluator.py:104)). This could be made configurable.
*   **Loss Threshold:** An example threshold for "high divergence" is hardcoded: `if sum(loss_components.values()) > 0.5:` ([`analyze_and_propose_rule_changes`](pipeline/evaluator.py:199)). This should ideally be a configurable parameter.

## 7. Coupling Points

*   **[`GPT.gpt_symbolic_convergence_loss`](GPT/gpt_symbolic_convergence_loss.py:1):** Tightly coupled for calculating loss metrics, which is central to the evaluation.
*   **[`intelligence.forecast_schema.ForecastSchema`](intelligence/forecast_schema.py:1):** Tightly coupled as it defines the structure of `pulse_output` and `gpt_struct` that are being compared.
*   **Data Format (Parquet):** Assumes input data is in Parquet format ([`evaluate`](pipeline/evaluator.py:45)). Changes to this format would require code modification (acknowledged by a TODO).
*   **Output File (`proposed_rule_changes.json`):** The module produces a JSON file ([`pipeline/rule_proposals/proposed_rule_changes.json`](pipeline/rule_proposals/proposed_rule_changes.json:1)) that other parts of the system (e.g., a rule application module) would consume. The structure of this JSON is a coupling point.
*   **Implicit Simulation Engine:** The TODOs suggest an intended coupling with a simulation engine ([`__init__`](pipeline/evaluator.py:20), [`evaluate`](pipeline/evaluator.py:110)) that is not yet realized.

## 8. Existing Tests

*   No specific test file (e.g., `tests/test_evaluator.py` or `tests/pipeline/test_evaluator.py`) was found during the search. This indicates a lack of dedicated unit tests for this module.

## 9. Module Architecture and Flow

1.  **Initialization (`__init__`):** Currently a placeholder, intended for setting up resources like a simulation client.
2.  **Evaluation (`evaluate`):**
    *   Receives model information and a path to a feature dataset (Parquet file).
    *   Loads the dataset.
    *   Iterates through each row (forecast episode):
        *   Validates and parses `pulse_output` and `gpt_struct` into [`ForecastSchema`](intelligence/forecast_schema.py:1) objects.
        *   Computes `symbolic_convergence_loss` and `loss_components` using functions from [`GPT.gpt_symbolic_convergence_loss`](GPT/gpt_symbolic_convergence_loss.py:1).
        *   Calls [`analyze_and_propose_rule_changes`](pipeline/evaluator.py:120) with the forecasts and loss components.
        *   Collects all proposed rule changes.
    *   Calculates an average `total_symbolic_convergence_loss`.
    *   Saves all collected proposed rule changes to [`pipeline/rule_proposals/proposed_rule_changes.json`](pipeline/rule_proposals/proposed_rule_changes.json:1).
    *   Returns a dictionary of basic metrics.
3.  **Rule Proposal (`analyze_and_propose_rule_changes`):**
    *   Receives Pulse forecast, GPT forecast, and loss components.
    *   Analyzes divergence in actions, state, and rationale.
    *   Based on detected divergences and a simple total loss threshold, it generates a list of dictionaries, each representing a proposed rule change (e.g., add, prune, modify) with reasons and details.

## 10. Naming Conventions

*   **Classes:** `Evaluator` ([`pipeline/evaluator.py:15`](pipeline/evaluator.py:15)) - Follows PascalCase (PEP 8).
*   **Methods:** `__init__`, `evaluate`, `analyze_and_propose_rule_changes` - Follow snake_case (PEP 8).
*   **Variables:** `model_info`, `feature_path`, `metrics`, `forecast_data`, `total_loss`, `num_forecasts`, `all_proposed_changes`, `pulse_forecast`, `gpt_forecast`, `loss_components`, `output_dir`, `output_path` - Generally follow snake_case (PEP 8).
*   **Constants/Hardcoded Strings:** `pipeline/rule_proposals` ([`evaluate`](pipeline/evaluator.py:102)), `proposed_rule_changes.json` ([`evaluate`](pipeline/evaluator.py:104)) are descriptive.
*   **Consistency:** Naming conventions appear consistent and largely adhere to PEP 8. No obvious AI assumption errors or significant deviations were noted.