{
  "pulse/config/loader.py": {
    "description": "Loads and merges configuration from YAML, .env, and environment variables, supporting nested keys, type casting, and override hierarchy.",
    "issues": []
  },
  "pulse/config/__main__.py": {
    "description": "CLI entry point for configuration inspection and testing.",
    "issues": []
  },
  "pulse/config/__init__.py": {
    "description": "Initializes the config package and exposes the `Config` loader.",
    "issues": []
  },
  "GPT/gpt_causal_translator.py": {
    "description": "Translates natural language causal statements into formal rule fingerprints using an LLM",
    "issues": []
  },
  "GPT/gpt_forecast_divergence_logger.py": {
    "description": "Logs forecast divergences identified by an LLM to a JSONL file",
    "issues": []
  },
  "GPT/gpt_rule_fingerprint_extractor.py": {
    "description": "Extracts rule fingerprints from natural language descriptions using an LLM",
    "issues": []
  },
  "GPT/gpt_symbolic_convergence_loss.py": {
    "description": "Calculates a symbolic convergence loss score for forecasts using an LLM",
    "issues": []
  },
  "__init__.py": {
    "description": "Initializes the root 'pulse' directory as a Python package, enabling module imports from within the project.",
    "issues": []
  },
  "adapters/core_adapter.py": {
    "description": "Provides an adapter for core configuration management, implementing the CoreInterface to offer a standardized way to load and access application configurations.",
    "issues": []
  },
  "adapters/simulation_adapter.py": {
    "description": "Provides an adapter for the simulation engine, implementing the `SimulationInterface` to standardize interactions and decouple the core simulation logic from its consumers.",
    "issues": []
  },
  "adapters/symbolic_adapter.py": {
    "description": "Adapts functionalities of the symbolic system, including symbolic upgrades, forecast rewriting, trace generation, mutation logging, and alignment computation, by implementing the `SymbolicInterface`.",
    "issues": []
  },
  "adapters/trust_adapter.py": {
    "description": "Provides an adapter for the trust system, implementing `TrustInterface` to offer standardized methods for forecast tagging, confidence gating, scoring, integrity checks, conflict detection, and trust metadata enrichment.",
    "issues": []
  },
  "analyze_historical_data_quality.py": {
    "description": "Analyzes historical data quality for retrodiction, checking completeness, depth, and generating reports",
    "issues": []
  },
  "api/core_api.py": {
    "description": "Provides a Flask-based RESTful API for interacting with core system functionalities, including forecasts, retrodiction, variable data, (placeholder) autopilot controls, and training review submissions.",
    "issues": []
  },
  "api/server.py": {
    "description": "Provides a Flask-based backend API for the Pulse Desktop UI, handling status, forecasts, autopilot control, retrodiction simulations, and learning audits. Includes a compatibility mode with simulated data if core Pulse modules are unavailable.",
    "issues": []
  },
  "api_key_report.py": {
    "description": "Tests API key accessibility (FRED, Finnhub, NASDAQ) via environment variables and reports status",
    "issues": []
  },
  "api_key_test.py": {
    "description": "Tests FRED, Finnhub, and NASDAQ API key accessibility and validity using two naming conventions, providing a console report",
    "issues": []
  },
  "api_key_test_updated.py": {
    "description": "An enhanced API key testing script with more detailed error reporting, multi-endpoint testing for NASDAQ, and structured return values from test functions",
    "issues": []
  },
  "benchmark_retrodiction.py": {
    "description": "Benchmarks the retrodiction training process, including data loading, causal discovery, and trust updates",
    "issues": []
  },
  "capital_engine/__init__.py": {
    "description": "An empty `__init__.py` file, marking the `capital_engine` directory as a Python package.",
    "issues": []
  },
  "capital_engine/capital_digest_formatter.py": {
    "description": "Formats capital forecast data, portfolio snapshots, and alignment tags into human-readable Markdown summaries, suitable for digests and operator views.",
    "issues": []
  },
  "capital_engine/capital_layer.py": {
    "description": "Description: The `capital_layer.py` module serves as a Unified Capital Simulation Layer. It translates symbolic system states (e.g., hope, despair) into simulated capital adjustments for financial assets.",
    "issues": []
  },
  "capital_engine/capital_layer_cli.py": {
    "description": "A command-line interface (CLI) script for demonstrating and testing the `capital_layer` module's shortview forecast, exposure summarization, and alignment tag functionalities using a mock `WorldState`.",
    "issues": []
  },
  "causal_model/counterfactual_engine.py": {
    "description": "Provides an engine for generating and evaluating counterfactual scenarios based on a structural causal model",
    "issues": []
  },
  "causal_model/counterfactual_simulator.py": {
    "description": "Provides a framework for running counterfactual simulations using a structural causal model, handling scenario management and execution, but has a placeholder for causal discovery",
    "issues": []
  },
  "causal_model/discovery.py": {
    "description": "Acts as an interface for causal discovery algorithms (PC, FCI), aiming for optimized implementations but currently relying on incomplete fallback methods",
    "issues": [
      "NotImplementedError (Line 104): except (NotImplementedError, AttributeError) as e: (File: causal_model/discovery.py)"
    ]
  },
  "causal_model/optimized_discovery.py": {
    "description": "Offers an optimized, vectorized PC algorithm implementation with parallel processing, though the core independence testing and edge orientation use simplified heuristics",
    "issues": []
  },
  "causal_model/structural_causal_model.py": {
    "description": "Provides a foundational `StructuralCausalModel` class using `networkx` to represent SCMs as DAGs, supporting basic graph operations; it's concise and appears complete for its core purpose",
    "issues": []
  },
  "causal_model/vectorized_operations.py": {
    "description": "Offers an optimized, vectorized functions for causal modeling calculations like correlation matrices and conditional independence tests, aiming to improve performance; it's partially complete with areas for refinement in statistical methods and graph query support",
    "issues": []
  },
  "chatmode/config/llm_config.py": {
    "description": "Manages LLM settings, securely handling API keys via environment variables, and allowing model selection overrides",
    "issues": []
  },
  "chatmode/conversational_core.py": {
    "description": "Central orchestrator for user interactions, handling intent, parameters, command execution, and LLM/RAG processing",
    "issues": []
  },
  "chatmode/integrations/pulse_module_adapters.py": {
    "description": "Key integration layer connecting `chatmode` to core Pulse systems; some placeholder logic",
    "issues": []
  },
  "chatmode/launch_conversational_ui.py": {
    "description": "This module serves as the entry point for the Pulse Conversational Interface GUI, handling argument parsing for LLM configuration and launching the application",
    "issues": []
  },
  "chatmode/llm_integration/domain_adapter.py": {
    "description": "Provides a `DomainAdapter` class for applying LoRA to language models for domain-specific fine-tuning",
    "issues": []
  },
  "chatmode/llm_integration/llm_model.py": {
    "description": "Foundational LLM interaction layer, with robust OpenAI/mock support but placeholder HuggingFace/local model implementations",
    "issues": [
      "TODO (Line 119): # TODO: Implement actual model loading logic (File: chatmode/llm_integration/llm_model.py)",
      "TODO (Line 154): # TODO: Implement LoRA adapter application logic (File: chatmode/llm_integration/llm_model.py)",
      "TODO (Line 352): # TODO: Implement actual generation logic for local models (File: chatmode/llm_integration/llm_model.py)"
    ]
  },
  "chatmode/llm_integration/openai_config.py": {
    "description": "Manages OpenAI API keys, model selection, and cost estimation",
    "issues": []
  },
  "chatmode/rag/__init__.py": {
    "description": "Marks directory as Python package.",
    "issues": []
  },
  "chatmode/rag/context_provider.py": {
    "description": "Well-structured module for RAG context retrieval using a vector store",
    "issues": []
  },
  "chatmode/test_openai_integration.py": {
    "description": "Provides essential integration tests for OpenAI API connectivity and basic LLM calls",
    "issues": []
  },
  "chatmode/ui/conversational_gui.py": {
    "description": "Comprehensive Tkinter-based GUI for user interaction with Pulse AI",
    "issues": []
  },
  "chatmode/vector_store/build_vector_store.py": {
    "description": "Well-structured module for creating, managing, and testing a FAISS-based vector store for RAG",
    "issues": []
  },
  "chatmode/vector_store/codebase_parser.py": {
    "description": "Effectively scans and chunks codebase files for vector store ingestion",
    "issues": []
  },
  "chatmode/vector_store/codebase_vector_store.py": {
    "description": "Clear, in-memory vector store implementation using sentence-transformers and FAISS; lacks persistence",
    "issues": []
  },
  "check_benchmark_deps.py": {
    "description": "Checks for required/optional dependencies for retrodiction benchmarking; exits if required are missing",
    "issues": []
  },
  "cli/gui_launcher.py": {
    "description": "Successfully launches the Pulse Desktop UI and its backend API, handling dependency checks and process management; key improvements include externalizing configurations like API URLs and enhancing testability and error reporting for subprocesses.",
    "issues": []
  },
  "cli/interactive_shell.py": {
    "description": "Provides a strategist shell for Pulse interaction with several unimplemented commands marked as stubs, indicating planned but incomplete functionality.",
    "issues": []
  },
  "cli/main.py": {
    "description": "Reveals it as the central CLI entry point for Pulse, managing simulations and data repair tasks via `argparse`.",
    "issues": []
  },
  "config/ai_config.py": {
    "description": "Securely configures OpenAI API access by loading the key from an environment variable and setting a default model; it's simple, focused, and designed for future AI service additions.",
    "issues": []
  },
  "config/gravity_config.py": {
    "description": "Provides a comprehensive set of hardcoded default constants for the Residual-Gravity Overlay system, covering core parameters, safety thresholds, feature flags, and adaptive behaviors, ensuring centralized and well-organized system tuning.",
    "issues": []
  },
  "conftest.py": {
    "description": "Defines basic Pytest fixtures for test suite consistency",
    "issues": []
  },
  "core/__init__.py": {
    "description": "Indicates it's currently an empty file, serving only to mark the `core/` directory as a Python package.",
    "issues": []
  },
  "core/bayesian_trust_tracker.py": {
    "description": "Provides a robust, thread-safe mechanism for tracking rule/variable reliability using Beta distributions, supporting updates, decay, persistence, and reporting.",
    "issues": []
  },
  "core/celery_app.py": {
    "description": "Configures Celery for distributed signal ingestion/scoring, integrating `IrisScraper`, `trust_system`, and `forecast_engine`.",
    "issues": []
  },
  "core/event_bus.py": {
    "description": "Implements a simple, in-memory publish-subscribe event system for decoupled communication.",
    "issues": []
  },
  "core/feature_store.py": {
    "description": "Provides a robust, configuration-driven `FeatureStore` for managing raw and engineered features with caching, though it could benefit from more granular transform dependencies and enhanced error handling.",
    "issues": []
  },
  "core/metrics.py": {
    "description": "Establishes basic Prometheus metrics for signal ingestion and trust scores, and includes a utility to start a metrics server, forming a good but minimal foundation for application monitoring.",
    "issues": []
  },
  "core/module_registry.py": {
    "description": "Contains only a comment indicating the module was removed and is no longer needed, signifying its obsolescence.",
    "issues": []
  },
  "core/optimized_trust_tracker.py": {
    "description": "Provides a high-performance, thread-safe Bayesian trust tracker using batch operations, NumPy, and caching.",
    "issues": []
  },
  "core/path_registry.py": {
    "description": "Primarily serves as a centralized file path management system for the Pulse application using Python's `pathlib` library.",
    "issues": []
  },
  "core/pulse_config.py": {
    "description": "Serves as a centralized configuration hub that defines system-wide constants, manages runtime flags, and provides utilities for loading/managing configuration from YAML and JSON files.",
    "issues": []
  },
  "core/pulse_learning_log.py": {
    "description": "Provides a robust singleton logger for Pulse's structural learning events, writing timestamped JSONL entries for diagnostics and audit.",
    "issues": []
  },
  "core/schemas.py": {
    "description": "Defines essential Pydantic models for data validation and structure within Pulse, covering forecasts and various log types.",
    "issues": []
  },
  "core/service_registry.py": {
    "description": "Provides a simple and effective service locator for core Pulse interfaces, promoting loose coupling.",
    "issues": []
  },
  "core/training_review_store.py": {
    "description": "Manages file-based storage and retrieval of forecast/retrodiction training submissions with an in-memory index.",
    "issues": []
  },
  "core/trust_update_buffer.py": {
    "description": "Implements a thread-safe buffer for batching trust updates to improve performance by reducing lock contention on the `OptimizedBayesianTrustTracker`.",
    "issues": []
  },
  "core/variable_accessor.py": {
    "description": "Provides safe getter/setter functions for worldstate variables and overlays, validating against a central registry; it is mostly complete but lacks implemented logging for unknown variables.",
    "issues": []
  },
  "core/variable_registry.py": {
    "description": "Acts as a central variable intelligence layer, managing static definitions, runtime values, and forecasting hooks for a vast array of economic and market variables; key improvements include refactoring the large initial static registry and fully implementing advertised features like trust tracking.",
    "issues": []
  },
  "dags/retrodiction_dag.py": {
    "description": "Defines an Airflow DAG for daily historical retrodiction tests, calling `engine.historical_retrodiction_runner.run_retrodiction_tests()`.",
    "issues": []
  },
  "data/manual_ingestion.py": {
    "description": "Processes a hardcoded list of local ZIP/CSV files, performs data type optimizations, and ingests them using `StreamingDataStore`; its utility is limited by this hardcoded nature.",
    "issues": []
  },
  "dev_tools/analysis/enhanced_phantom_scanner.py": {
    "description": "Provides a static analyzer, `EnhancedPhantomScanner`, to detect and categorize functions called but not defined or imported within a Python codebase, using AST parsing and offering contextual reports.",
    "issues": []
  },
  "dev_tools/analysis/phantom_function_scanner.py": {
    "description": "Analyzes Python code to find function calls that lack local definitions, aiding in code cleanup and error prevention. It uses AST parsing for static analysis but currently doesn't resolve imports, potentially leading to false positives.",
    "issues": []
  },
  "dev_tools/apply_symbolic_revisions.py": {
    "description": "Applies symbolic revisions based on a plan.",
    "issues": []
  },
  "dev_tools/apply_symbolic_upgrades.py": {
    "description": "CLI tool that applies a single symbolic upgrade plan to an entire batch of forecasts, saving rewritten forecasts and logging mutations. Depends on `symbolic_system.symbolic_executor`.",
    "issues": []
  },
  "dev_tools/certify_forecast_batch.py": {
    "description": "Certifies a batch of forecasts for quality and reliability.",
    "issues": []
  },
  "dev_tools/cli_retrodict_forecasts.py": {
    "description": "Functional command-line tool for applying retrodiction scoring to forecasts, relying on `analytics.analytics.retrospective_analysis_batch()`.",
    "issues": []
  },
  "dev_tools/cli_trace_audit.py": {
    "description": "Provides a command-line interface for the `analytics.trace_audit_engine` to replay, summarize, or audit traces.",
    "issues": []
  },
  "dev_tools/compress_forecast_chain.py": {
    "description": "Functional CLI tool for compressing a forecast mutation chain, relying on imported modules for core logic and lacking dedicated tests.",
    "issues": []
  },
  "dev_tools/enforce_forecast_batch.py": {
    "description": "CLI tool that applies license rules to forecast batches by leveraging the `trust_system.license_enforcer` module. Functional and well-structured, but needs dedicated tests, robust error handling, and formal logging.",
    "issues": []
  },
  "dev_tools/episode_memory_viewer.py": {
    "description": "CLI utility to explore and visualize symbolic episode memory by summarizing arcs/tags, plotting arc frequency, and exporting summaries.",
    "issues": []
  },
  "dev_tools/epistemic_mirror_review.py": {
    "description": "A command-line utility that processes and summarizes \"foreign causal fingerprints\" and \"GPT forecast divergence logs\" from specified JSONL files, providing console output or a Markdown export for operator review and analysis.",
    "issues": []
  },
  "dev_tools/generate_plugin_stubs.py": {
    "description": "Developer utility that successfully automates the creation of idempotent, boilerplate Iris plugin files, facilitating faster onboarding for new plugin development.",
    "issues": [
      "TODO (Line 66): # TODO: implement real fetch + formatting (File: dev_tools/generate_plugin_stubs.py)"
    ]
  },
  "dev_tools/generate_symbolic_upgrade_plan.py": {
    "description": "Command-line tool to orchestrate the generation and export of symbolic system upgrade plans by processing tuning result logs through various `symbolic_system` components.",
    "issues": []
  },
  "dev_tools/hook_utils.py": {
    "description": "Scans directories for Python modules with CLI hook candidates (main/run functions or `__hook__` tag) and returns metadata for CLI integration; functionally complete but lacks dedicated tests and has some hardcoded values.",
    "issues": []
  },
  "dev_tools/log_forecast_audits.py": {
    "description": "Processes forecast batches to generate and log audit trail entries, relying on `trust_system.forecast_audit_trail` for core logic.",
    "issues": []
  },
  "dev_tools/memory_recovery_viewer.py": {
    "description": "CLI tool to explore, summarize, and export discarded forecasts from license enforcement gates, appearing functionally complete for its scope.",
    "issues": []
  },
  "dev_tools/module_dependency_map.py": {
    "description": "Analyzes Python import dependencies, prints a summary, and exports the map to a Graphviz DOT file; it is functional but lacks tests and could be enhanced to parse direct `import` statements and offer more configurable exclusions.",
    "issues": []
  },
  "dev_tools/operator_brief_cli.py": {
    "description": "Provides a command-line interface to generate markdown-based Operator Briefs from forecast alignment and episode logs, and can optionally explain forecast license decisions, relying on `operator_interface` and `trust_system` components.",
    "issues": []
  },
  "dev_tools/propose_epistemic_upgrades.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_arc_cli.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_argument_checker.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_autoscan_on_add.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_batch_alignment_analyzer.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_cli_dashboard.py": {
    "description": "Well-structured utility for displaying available Pulse CLI modes, enhancing developer experience by improving command discoverability. It reads a configuration file, groups and color-codes modes, and allows filtering, though an unused configuration variable was noted.",
    "issues": []
  },
  "dev_tools/pulse_cli_docgen.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_code_validator.py": {
    "description": "Useful static analysis tool for detecting keyword argument mismatches in Python function calls across the project. It parses code using `ast` and provides configurable reporting, though its function resolution and handling of `**kwargs` could be enhanced for greater accuracy.",
    "issues": []
  },
  "dev_tools/pulse_dir_cleaner.py": {
    "description": "Helps organize the project by identifying duplicate or misplaced Python files (based on a predefined list of canonical paths) and moving them to a quarantine directory, keeping the most recently modified version in the correct location. Its reliance on a hardcoded list of canonical paths might be a scalability concern.",
    "issues": []
  },
  "dev_tools/pulse_encoding_checker.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_forecast_evaluator.py": {
    "description": "Evaluates the performance of forecasts using various metrics.",
    "issues": []
  },
  "dev_tools/pulse_forecast_test_suite.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_scan_hooks.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_shell_autohook.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_signal_router_v2_cli.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/pulse_test_suite.py": {
    "description": "The dev_tools/pulse_test_suite.py module provides basic \"smoke tests\" to confirm that symbolic overlays and capital exposure change during simulation, but it lacks formal assertions and detailed validation of the changes' correctness.",
    "issues": []
  },
  "dev_tools/pulse_ui_bridge.py": {
    "description": "The dev_tools/pulse_ui_bridge.py module effectively connects CLI tools for recursion audits, brief generation, and variable plotting to a UI, featuring Tkinter-based helpers for user interaction, though its error handling could be more robust for UI integration.",
    "issues": []
  },
  "dev_tools/pulse_ui_plot.py": {
    "description": "The dev_tools/pulse_ui_plot.py module offers a functional CLI and library for visualizing Pulse simulation variable trends and alignment scores using matplotlib, with good error handling for data loading, though plot customization is limited.",
    "issues": []
  },
  "dev_tools/pulse_ui_replay.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/rule_audit_viewer.py": {
    "description": "The dev_tools/rule_audit_viewer.py module is a CLI tool that effectively displays rule-induced changes from forecast JSON files, aiding in debugging and analysis. It is functional but could benefit from dedicated unit tests and more robust error handling.",
    "issues": []
  },
  "dev_tools/rule_dev_shell.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/run_symbolic_learning.py": {
    "description": "The module is a concise CLI script that triggers the symbolic learning process using a tuning log. It delegates all core logic to the symbolic_system.pulse_symbolic_learning_loop module. It is functional but could be improved with explicit error handling and user feedback.",
    "issues": []
  },
  "dev_tools/run_symbolic_sweep.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/symbolic_drift_plot.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "dev_tools/symbolic_flip_analyzer.py": {
    "description": "is a CLI tool for analyzing symbolic state transition patterns and loops within forecast episodes, depending on `analytics.forecast_episode_tracer` and `symbolic_system.symbolic_flip_classifier`.",
    "issues": []
  },
  "dev_tools/testing/api_key_test.py": {
    "description": "Tests API key access (FRED, Finnhub, NASDAQ) via env vars, reports status. Good quality.",
    "issues": []
  },
  "dev_tools/testing/api_key_test_updated.py": {
    "description": "Description: An enhanced API key testing script with more detailed error reporting, multi-endpoint testing for NASDAQ, and structured return values from test functions, making it a robust diagnostic tool.",
    "issues": []
  },
  "dev_tools/testing/conftest.py": {
    "description": "Description: This Pytest configuration file provides a session-scoped mock API key (`\"test_api_key_12345\"`) for tests within the `dev_tools/testing/` directory, simplifying test setup and promoting isolation.",
    "issues": []
  },
  "dev_tools/utils/delete_pycache.py": {
    "description": "Utility to delete __pycache__ directories recursively.",
    "issues": []
  },
  "dev_tools/utils/git_cleanup.py": {
    "description": "Description: Git repository cleanup.",
    "issues": []
  },
  "dev_tools/utils/patch_imports.py": {
    "description": "Description: Patching import statements.",
    "issues": []
  },
  "dev_tools/visualize_symbolic_graph.py": {
    "description": "Description: The `dev_tools/visualize_symbolic_graph.py` module is a CLI tool that loads forecast data to build and visualize a symbolic transition graph, primarily relying on functions from `symbolic_system.symbolic_transition_graph`.",
    "issues": []
  },
  "diagnostics/gravity_explainer.py": {
    "description": "Explains the gravity model outputs.",
    "issues": []
  },
  "diagnostics/shadow_model_monitor.py": {
    "description": "Description: Monitors the 'gravity model's' influence on simulations. Supports understanding, debugging, and monitoring the Pulse system's modeling components.",
    "issues": []
  },
  "enhanced_phantom_scanner.py": {
    "description": "Provides a static analyzer, `EnhancedPhantomScanner`, to detect and categorize functions called but not defined or imported within a Python codebase",
    "issues": []
  },
  "examples/context7_integration_example.py": {
    "description": "Demonstrates integration with Context7.",
    "issues": []
  },
  "examples/historical_timeline_example.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "examples/historical_verification_example.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "examples/mlflow_tracking_example.py": {
    "description": "Description: Minimal, standalone script demonstrating basic MLflow logging. Serves as an illustrative example of MLflow integration.",
    "issues": []
  },
  "examples/symbolic_overlay_demo.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "facades/system_facade.py": {
    "description": "Provides a simplified interface to complex subsystems.",
    "issues": []
  },
  "forecast_engine/__init__.py": {
    "description": "Initializes the forecast engine package.",
    "issues": []
  },
  "forecast_engine/ai_forecaster.py": {
    "description": "Implements AI-based forecasting algorithms.",
    "issues": []
  },
  "forecast_engine/ensemble_manager.py": {
    "description": "Manages multiple forecast models in an ensemble.",
    "issues": [
      "TODO (Line 98): # TODO: implement boosting algorithm (e.g., AdaBoost or GradientBoosting) (File: forecast_engine/ensemble_manager.py)"
    ]
  },
  "forecast_engine/forecast_batch_runner.py": {
    "description": "Runs forecasting tasks in batches.",
    "issues": []
  },
  "forecast_engine/forecast_compressor.py": {
    "description": "Compresses forecast data for efficient storage.",
    "issues": []
  },
  "forecast_engine/forecast_drift_monitor.py": {
    "description": "Monitors forecast accuracy and detects model drift.",
    "issues": []
  },
  "forecast_engine/forecast_ensemble.py": {
    "description": "Defines the structure for forecast ensembles.",
    "issues": []
  },
  "forecast_engine/forecast_exporter.py": {
    "description": "Exports forecast results to various formats.",
    "issues": []
  },
  "forecast_engine/forecast_integrity_engine.py": {
    "description": "Ensures the integrity and consistency of forecast data.",
    "issues": [
      "TODO (Line 58): # TODO: Integrate with do-calculus or causal graph library (File: forecast_engine/forecast_integrity_engine.py)"
    ]
  },
  "forecast_engine/forecast_log_viewer.py": {
    "description": "Provides tools to view and analyze forecast logs.",
    "issues": []
  },
  "forecast_engine/forecast_memory.py": {
    "description": "Manages historical forecast data for learning.",
    "issues": []
  },
  "forecast_engine/forecast_regret_engine.py": {
    "description": "Calculates regret for forecast decisions.",
    "issues": [
      "TODO (Line 92): # TODO: Integrate with model update logic (File: forecast_engine/forecast_regret_engine.py)"
    ]
  },
  "forecast_engine/forecast_scoring.py": {
    "description": "Scores forecasts based on accuracy metrics.",
    "issues": []
  },
  "forecast_engine/forecast_tools.py": {
    "description": "Contains utility functions for the forecast engine.",
    "issues": []
  },
  "forecast_engine/forecast_tracker.py": {
    "description": "Tracks the status and progress of forecasting tasks.",
    "issues": []
  },
  "forecast_engine/hyperparameter_tuner.py": {
    "description": "Tunes hyperparameters for forecasting models.",
    "issues": []
  },
  "forecast_engine/simulation_prioritizer.py": {
    "description": "Prioritizes simulations based on forecast impact.",
    "issues": []
  },
  "forecast_output/__init__.py": {
    "description": "Initializes the forecast output package.",
    "issues": []
  },
  "forecast_output/cluster_memory_compressor.py": {
    "description": "Compresses data related to cluster memory.",
    "issues": []
  },
  "forecast_output/digest_exporter.py": {
    "description": "Exports forecast digests.",
    "issues": [
      "TODO (Line 82): # TODO: Integrate ReportLab or WeasyPrint for real PDF export (File: forecast_output/digest_exporter.py)"
    ]
  },
  "forecast_output/digest_logger.py": {
    "description": "Logs information related to digest generation.",
    "issues": []
  },
  "forecast_output/digest_trace_hooks.py": {
    "description": "Provides hooks for tracing digest creation.",
    "issues": []
  },
  "forecast_output/dual_narrative_compressor.py": {
    "description": "Compresses dual narrative forecast outputs.",
    "issues": []
  },
  "forecast_output/forecast_age_tracker.py": {
    "description": "Tracks the age of forecasts.",
    "issues": []
  },
  "forecast_output/forecast_cluster_classifier.py": {
    "description": "Classifies forecasts into clusters.",
    "issues": []
  },
  "forecast_output/forecast_compressor.py": {
    "description": "Compresses final forecast outputs.",
    "issues": []
  },
  "forecast_output/forecast_confidence_gate.py": {
    "description": "Gates forecasts based on confidence levels.",
    "issues": []
  },
  "forecast_output/forecast_conflict_resolver.py": {
    "description": "Resolves conflicts between different forecasts.",
    "issues": []
  },
  "forecast_output/forecast_contradiction_detector.py": {
    "description": "Detects contradictions in forecast data.",
    "issues": []
  },
  "forecast_output/forecast_contradiction_digest.py": {
    "description": "Creates digests of forecast contradictions.",
    "issues": []
  },
  "forecast_output/forecast_divergence_detector.py": {
    "description": "Detects divergence in forecasts over time.",
    "issues": []
  },
  "forecast_output/forecast_fidelity_certifier.py": {
    "description": "Certifies the fidelity of forecasts.",
    "issues": []
  },
  "forecast_output/forecast_formatter.py": {
    "description": "Formats forecasts for output and display.",
    "issues": []
  },
  "forecast_output/forecast_generator.py": {
    "description": "Generates final forecast outputs from processed data.",
    "issues": []
  },
  "forecast_output/forecast_licenser.py": {
    "description": "Manages licensing for forecast outputs.",
    "issues": []
  },
  "forecast_output/forecast_memory_promoter.py": {
    "description": "Promotes important forecasts to long-term memory.",
    "issues": []
  },
  "forecast_output/forecast_pipeline_cli.py": {
    "description": "Provides a CLI for the forecast output pipeline.",
    "issues": []
  },
  "forecast_output/forecast_prioritization_engine.py": {
    "description": "Prioritizes forecasts for output and action.",
    "issues": []
  },
  "forecast_output/forecast_resonance_scanner.py": {
    "description": "Scans forecasts for resonance with known patterns.",
    "issues": []
  },
  "forecast_output/forecast_summary_synthesizer.py": {
    "description": "Synthesizes summaries from detailed forecasts.",
    "issues": []
  },
  "forecast_output/forecast_tags.py": {
    "description": "Manages tags associated with forecasts.",
    "issues": []
  },
  "forecast_output/mutation_compression_engine.py": {
    "description": "Compresses mutation data related to forecasts.",
    "issues": []
  },
  "forecast_output/pfpa_logger.py": {
    "description": "Logs PFPA (Pulse Forecast Performance Analysis) data.",
    "issues": []
  },
  "forecast_output/pulse_converge.py": {
    "description": "Manages convergence of forecast signals.",
    "issues": []
  },
  "forecast_output/pulse_forecast_lineage.py": {
    "description": "Tracks the lineage of forecasts.",
    "issues": []
  },
  "forecast_output/strategic_fork_resolver.py": {
    "description": "Resolves strategic forks in forecast narratives.",
    "issues": []
  },
  "forecast_output/strategos_digest_builder.py": {
    "description": "Builds Strategos digests from forecasts.",
    "issues": []
  },
  "forecast_output/strategos_tile_formatter.py": {
    "description": "Formats forecast data into Strategos tiles.",
    "issues": []
  },
  "forecast_output/symbolic_tuning_engine.py": {
    "description": "Tunes symbolic parameters for forecast output.",
    "issues": []
  },
  "improve_historical_data.py": {
    "description": "Enhances historical financial/economic data for retrodiction, with a fragile direct plugin modification method",
    "issues": []
  },
  "intelligence/forecast_schema.py": {
    "description": "Description: Defines a Pydantic schema to validate the structure and data types of forecast dictionaries generated by the Pulse system.",
    "issues": []
  },
  "intelligence/function_router.py": {
    "description": "Description: Dynamically loads and routes function calls based on configured verbs, featuring retry/back-off logic for imports and centralized logging.",
    "issues": []
  },
  "intelligence/intelligence_config.py": {
    "description": "Description: Centralizes configuration constants and settings for the Pulse Intelligence system, covering components like the Function Router, Observer, and LLM integrations.",
    "issues": []
  },
  "intelligence/intelligence_core.py": {
    "description": "Description: Serves as the central orchestrator for Pulse simulation and learning cycles, managing interactions between key components like the Function Router, Simulation Executor, and Intelligence Observer.",
    "issues": []
  },
  "intelligence/intelligence_observer.py": {
    "description": "Description: Serves as the central learning intelligence layer for Pulse, observing divergences, proposing epistemic upgrades, and logging learning episodes.",
    "issues": []
  },
  "intelligence/intelligence_shell.py": {
    "description": "Description: This module provides a command-line interface for users to interact with and manage the Pulse Intelligence Core's functionalities.",
    "issues": []
  },
  "intelligence/simulation_executor.py": {
    "description": "Description: This module is responsible for executing simulation and retrodiction forecasts, integrating with function routing, LLM analysis, and forecast compression.",
    "issues": []
  },
  "intelligence/upgrade_sandbox_manager.py": {
    "description": "Description: This module manages epistemic upgrade proposals by receiving, storing, and allowing retrieval of these proposals before a trust-gated promotion process.",
    "issues": []
  },
  "intelligence/worldstate_loader.py": {
    "description": "Description: This module is responsible for loading and initializing WorldState objects from various sources like baseline files, live data, and historical snapshots.",
    "issues": []
  },
  "interfaces/core_interface.py": {
    "description": "Description: The analysis for this module concluded that it defines an abstract base class for configuration management, providing a foundational structure for handling application settings.",
    "issues": []
  },
  "interfaces/simulation_interface.py": {
    "description": "Description: The analysis for this module concluded that it defines an abstract base class for simulation environments, outlining methods for simulation setup, execution, and state management.",
    "issues": []
  },
  "interfaces/symbolic_interface.py": {
    "description": "Description: The analysis for this module concluded that it defines an abstract interface for symbolic reasoning components, outlining methods for processing, transformation, and evaluation of symbolic data.",
    "issues": []
  },
  "interfaces/trust_interface.py": {
    "description": "Description: The analysis for this module concluded that it defines an abstract base class for trust management systems, outlining methods for updating, querying, and evaluating trust scores or relationships.",
    "issues": []
  },
  "iris/": {
    "description": "Overview of the `iris/` directory and its contents.",
    "issues": []
  },
  "iris/check_env_vars.py": {
    "description": "Description: The primary role of the \\`iris/check_env_vars.py\\` module is to verify that all required environment variables, specifically API keys and credentials for various Pulse plugins, are set in the system's environment. It serves as a diagnostic tool to ensure the application has the necessary configurations to interact with external services.",
    "issues": []
  },
  "iris/conftest.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/high_frequency_indicators.py": {
    "description": "Description: Partially implemented module for calculating high-frequency technical indicators (MA, volume, volatility) with dependencies on \\`HighFrequencyDataAccess\\`, notable hardcoding issues, and a current lack of unit tests, but a clear structure for future expansion.",
    "issues": []
  },
  "iris/high_frequency_ingestion.py": {
    "description": "Description: Reveals a module for fetching and storing high-frequency stock data from Alpha Vantage, with rate limiting and basic data processing, but with opportunities for enhanced error handling, configuration, and testing.",
    "issues": []
  },
  "iris/ingest_api.py": {
    "description": "Description: Provides a production-ready FastAPI endpoint for ingesting signals via HTTP, dispatching them to Celery for asynchronous processing, and includes API key authentication and Prometheus metrics.",
    "issues": []
  },
  "iris/ingest_db.py": {
    "description": "Description: Shows a production-ready database polling module for signal ingestion via Celery, with configurable parameters and Prometheus metrics integration, though it has unused code elements and lacks dedicated tests.",
    "issues": []
  },
  "iris/ingest_fs.py": {
    "description": "Description: Reveals a production-ready module for ingesting JSON/CSV files from the filesystem via Celery, with Prometheus metrics, though it lacks dedicated tests and has an unused \\`IrisScraper\\` instance.",
    "issues": []
  },
  "iris/ingest_kafka.py": {
    "description": "Description: Shows it's a production-ready Kafka consumer that ingests signals and sends them to Celery, featuring error handling and metrics, but with an unused \\`IrisScraper\\` and no dedicated tests.",
    "issues": []
  },
  "iris/ingest_s3.py": {
    "description": "Description: Shows a production-ready S3 polling module for ingesting JSON/CSV files via Celery, with Prometheus metrics, but with an unused \\`IrisScraper\\` instance and no dedicated tests.",
    "issues": []
  },
  "iris/ingest_thirdparty.py": {
    "description": "Description: Reveals a functional Twitter ingestion module using Tweepy to fetch tweets and send them to Celery, with configuration via environment variables and basic error handling, but with an unused \\`IrisScraper\\` instance and opportunities for generalization and enhanced testing.",
    "issues": []
  },
  "iris/iris_archive.py": {
    "description": "Description: Shows a basic append-only JSONL archive for signals, functional for core operations but lacking advanced features, scalability for large archives, and dedicated tests.",
    "issues": []
  },
  "iris/iris_plugins.py": {
    "description": "Description: Shows it manages dynamic ingestion plugins, with good core functionality but areas for improvement in plugin discovery logic and dedicated unit testing for the manager class itself.",
    "issues": []
  },
  "iris/iris_plugins_finance.py": {
    "description": "Description: reveals a module for ingesting financial data from Alpha Vantage and Finnhub, with a documented but unimplemented FRED plugin, hardcoded symbol lists, and no dedicated unit tests.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/acled_plugin.py": {
    "description": "Description: reveals it is a non-operational stub module intended for ACLED API data ingestion, requiring full implementation of its core data fetching logic and lacking any dedicated tests.",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/acled_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/alpha_vantage_plugin.py": {
    "description": "Description: shows a largely complete and operational module for fetching financial data from Alpha Vantage, with good error handling and data persistence, but with areas for improvement in rate-limiting strategy, symbol configurability, and historical data ingestion.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/bls_plugin.py": {
    "description": "Description: shows a partially complete module for fetching and ingesting economic data from the BLS API, with several TODOs for comprehensive series coverage and historical data fetching.",
    "issues": [
      "TODO (Line 57): # TODO: Find and add BLS series IDs for inflation measures (e.g., CPI) (File: iris/iris_plugins_variable_ingestion/bls_plugin.py)",
      "TODO (Line 66): # TODO: Implement logic to fetch data for a comprehensive historical range. (File: iris/iris_plugins_variable_ingestion/bls_plugin.py)",
      "TODO (Line 119): # TODO: Find and add BLS series IDs for unemployment and labor force participation (File: iris/iris_plugins_variable_ingestion/bls_plugin.py)",
      "TODO (Line 128): # TODO: Implement logic to fetch data for a comprehensive historical range. (File: iris/iris_plugins_variable_ingestion/bls_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/cboe_plugin.py": {
    "description": "Description: shows it is a placeholder module for CBOE data ingestion, currently non-operational and requiring full implementation of data fetching, processing, and storage logic, with no dedicated tests.",
    "issues": [
      "TODO (Line 18): # TODO: Add logic here to check for credentials/access if applicable (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)",
      "TODO (Line 29): # TODO: Implement actual data fetching logic if API access is available. (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)",
      "TODO (Line 54): # TODO: Adapt this based on the actual CBOE data structure (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)",
      "TODO (Line 61): # TODO: Implement proper date parsing for the specific date format (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)",
      "TODO (Line 71): # TODO: Construct variable_name based on actual data/identifiers (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)",
      "TODO (Line 93): # TODO: Add calls to other ingest methods if more CBOE data types are added (File: iris/iris_plugins_variable_ingestion/cboe_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/cdc_socrata_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/cdc_socrata_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/census_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 20): self.api_key = os.environ.get(\"CENSUS_API_KEY\") # TODO: Add CENSUS_API_KEY configuration (File: iris/iris_plugins_variable_ingestion/census_plugin.py)",
      "TODO (Line 161): # TODO: Adapt this based on the actual Census API response structure and date format (File: iris/iris_plugins_variable_ingestion/census_plugin.py)",
      "TODO (Line 255): # TODO: Adapt this based on the actual Census API response structure and date format (File: iris/iris_plugins_variable_ingestion/census_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/cia_factbook_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/cia_factbook_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/coinmarketcap_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/data_portal_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/data_portal_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/financial_news_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/fred_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 7): # TODO: Add FRED API key configuration (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 43): # TODO: Find and add FRED series IDs for interest rates and yield curves (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 70): # TODO: Find and add FRED series IDs for industrial production (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 96): # TODO: Find and add FRED series IDs for unemployment and labor force participation (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 121): # TODO: Find and add FRED series IDs for money supply aggregates (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 146): # TODO: Find and add FRED series IDs for exchange rates (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 172): # TODO: Find and add FRED series IDs for credit spreads and volatility indices (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)",
      "TODO (Line 197): # TODO: Find and add FRED series IDs for housing starts and building permits (File: iris/iris_plugins_variable_ingestion/fred_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/gdelt_plugin.py": {
    "description": "Description: shows a functional GDELT API client for ingesting geopolitical event and media narrative data, with good error handling and data persistence, but with opportunities for enhanced query strategies, deeper GKG analysis, and dedicated testing.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/github_plugin.py": {
    "description": "Description: shows a functional GitHub API client for monitoring open-source trends, with areas for improvement in pagination handling and configuration management.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/google_trends_plugin.py": {
    "description": "Description: shows a largely complete module for fetching Google Trends data using `pytrends`, with good data persistence and error handling, but with areas for improvement in keyword/region rotation logic, keyword coverage, and dedicated testing.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/hackernews_plugin.py": {
    "description": "Description: shows a functional Hacker News API client for tracking tech trends and story scores, with areas for improvement in keyword configurability, sentiment analysis depth, and dedicated testing.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/healthmap_plugin.py": {
    "description": "Description: The HealthMap plugin ([`iris/iris_plugins_variable_ingestion/healthmap_plugin.py`](iris/iris_plugins_variable_ingestion/healthmap_plugin.py)) is a functional module for ingesting global health event data from HealthMap RSS feeds, featuring daily feed rotation and data persistence, though it has opportunities for more sophisticated data extraction and configuration management.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/high_frequency_indicator_plugin.py": {
    "description": "Description: functional plugin for ingesting high-frequency technical indicators, with key areas for improvement being accurate timestamping and enhanced error/symbol configuration.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/historical_ingestion_plugin.py": {
    "description": "Description: functional plugin for fetching real (FRED, Yahoo Finance) and generating synthetic historical data, with extensive hardcoded data source mappings and a lack of automated tests.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/ism_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 18): # TODO: Add logic here to check for credentials/access if applicable (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)",
      "TODO (Line 29): # TODO: Implement actual data fetching logic if API access is available. (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)",
      "TODO (Line 54): # TODO: Adapt this based on the actual ISM API response structure (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)",
      "TODO (Line 61): # TODO: Implement proper date parsing for ISM date format (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)",
      "TODO (Line 71): # TODO: Construct variable_name based on actual data/identifiers (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)",
      "TODO (Line 93): # TODO: Add calls to other ingest methods if more ISM data types are added (File: iris/iris_plugins_variable_ingestion/ism_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/manual_opec_plugin.py": {
    "description": "Description: functional plugin for ingesting OPEC data from a local zip file, with hardcoded paths and potential for incremental saving improvements.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/mediastack_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/mediastack_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/nasa_power_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/nasdaq_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/news_api_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/newsapi_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/noaa_cdo_plugin.py": {
    "description": "Description: TO DO",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/noaa_cdo_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/open_meteo_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/openaq_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/openfda_plugin.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/patentsview_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/patentsview_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/reddit_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/stackexchange_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/stackexchange_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/twitter_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/twitter_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 18): # TODO: Add logic here to check for credentials/access if applicable (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)",
      "TODO (Line 29): # TODO: Implement actual data fetching logic if API access is available. (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)",
      "TODO (Line 54): # TODO: Adapt this based on the actual data structure (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)",
      "TODO (Line 61): # TODO: Implement proper date parsing for the specific date format (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)",
      "TODO (Line 71): # TODO: Construct variable_name based on actual data/identifiers (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)",
      "TODO (Line 93): # TODO: Add calls to other ingest methods if more data types are added (File: iris/iris_plugins_variable_ingestion/umich_sentiment_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/vi_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/who_gho_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/wikidata_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/wolfram_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": [
      "TODO (Line 14): # TODO: implement real fetch + formatting (File: iris/iris_plugins_variable_ingestion/wolfram_plugin.py)"
    ]
  },
  "iris/iris_plugins_variable_ingestion/world_bank_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_plugins_variable_ingestion/worldbank_plugin.py": {
    "description": "Description: Not found in sprint0_analysis_report.md or individual module analysis file.",
    "issues": []
  },
  "iris/iris_scraper.py": {
    "description": "Description: orchestrates signal ingestion from plugins, applies trust scoring and symbolic tagging, and exports results, appearing largely complete but with opportunities for enhanced error handling and configuration management.",
    "issues": []
  },
  "iris/iris_symbolism.py": {
    "description": "Description: a module for symbolic tagging of signals using heuristics and an optional zero-shot model, with areas for improvement in configurability and testing.",
    "issues": []
  },
  "iris/iris_trust.py": {
    "description": "Description: a functional signal trust scoring module using recency and anomaly detection (Isolation Forest, Z-score) to compute a Signal Trust Index (STI), though it lacks dedicated tests and has several hardcoded parameters.",
    "issues": []
  },
  "iris/iris_utils/cli_historical_data.py": {
    "description": "Description: provides a comprehensive CLI for historical data management, integrating retrieval, transformation, verification, quality checks, repair, and reporting functionalities.",
    "issues": []
  },
  "iris/iris_utils/conftest.py": {
    "description": "Description: provides a simple pytest fixture for the ingestion.iris_utils package, appearing complete for its limited testing utility purpose.",
    "issues": []
  },
  "iris/iris_utils/historical_data_repair.py": {
    "description": "Historical Data Repair",
    "issues": []
  },
  "iris/iris_utils/historical_data_retriever.py": {
    "description": "Description: shows a functional module for fetching, analyzing, and storing historical financial/economic data from sources like FRED and Yahoo Finance, with areas for improvement in source extensibility and test coverage.",
    "issues": []
  },
  "iris/iris_utils/historical_data_transformer.py": {
    "description": "Description: shows a module for transforming raw historical data into a standardized format and storing it, with good core functionality but areas for improvement in error handling, data validation, and test coverage.",
    "issues": []
  },
  "iris/iris_utils/historical_data_verification.py": {
    "description": "Description: The [`historical_data_verification.py`](iris/iris_utils/historical_data_verification.py:1) module provides comprehensive tools for historical time-series data quality assurance, including anomaly/gap/trend detection, cross-source validation, and quality scoring, with some visualization capabilities and minor unimplemented features.",
    "issues": []
  },
  "iris/iris_utils/ingestion_persistence.py": {
    "description": "Description: shows a reusable module for standardized data persistence from API ingestion plugins, covering directory management, data saving (JSON, CSV, JSONL), metadata storage, and sensitive data masking, though it lacks dedicated tests and has some hardcoded configurations.",
    "issues": []
  },
  "iris/iris_utils/test_historical_data_pipeline.py": {
    "description": "Description: Analysis of [`iris/iris_utils/test_historical_data_pipeline.py`](iris/iris_utils/test_historical_data_pipeline.py:1) shows it's an end-to-end integration test for the historical data pipeline, validating data retrieval, transformation, storage, and verification for a sample set of variables.",
    "issues": []
  },
  "iris/iris_utils/world_bank_integration.py": {
    "description": "Description: Analysis of [`iris/iris_utils/world_bank_integration.py`](iris/iris_utils/world_bank_integration.py:1) shows a module for integrating World Bank bulk data, including processing, transformation, storage via a custom `RecursiveDataStore` wrapper, and variable catalog updates, with a CLI for execution.",
    "issues": []
  },
  "iris/pulse_signal_router_v2.py": {
    "description": "Description: Analysis of [`iris/pulse_signal_router_v2.py`](../iris/pulse_signal_router_v2.py) shows a functional signal routing module that integrates with core IRIS components for symbolic tagging, trust scoring, and archiving, with clear paths for future expansion but lacking dedicated tests.",
    "issues": []
  },
  "iris/retrieve_historical_data.py": {
    "description": "Description: Analysis of [`iris/retrieve_historical_data.py`](../iris/retrieve_historical_data.py:1) and its core logic in [`iris/iris_utils/historical_data_retriever.py`](../iris/iris_utils/historical_data_retriever.py:1) shows a CLI tool for fetching, analyzing, and storing historical financial/economic data from sources like FRED and Yahoo Finance, with areas for improvement in source extensibility and test coverage.",
    "issues": []
  },
  "iris/signal_gating.py": {
    "description": "Description: Analysis of [`iris/signal_gating.py`](iris/signal_gating.py:1) shows a functional symbolic trust filter for signals, with configurable rules and `PulseGrow` escalation, but with in-memory anomaly counting and some hardcoded thresholds.",
    "issues": []
  },
  "iris/test_alpha_vantage.py": {
    "description": "Description: The `iris/test_alpha_vantage.py` module is a basic test script for the Alpha Vantage plugin, verifying API connectivity and data fetching.",
    "issues": []
  },
  "iris/test_github.py": {
    "description": "Description: The `iris/test_github.py` module is a basic test script for the GitHub plugin, verifying API connectivity and data fetching.",
    "issues": []
  },
  "iris/test_newsapi_direct.py": {
    "description": "Description: The `iris/test_newsapi_direct.py` module is a test script for the NewsAPI plugin that requires manual API key input and has basic test coverage.",
    "issues": []
  },
  "iris/test_open_meteo.py": {
    "description": "Description: The `iris/test_open_meteo.py` module is a basic test script for the Open-Meteo plugin, verifying API connectivity and data fetching.",
    "issues": []
  },
  "iris/test_plugins.py": {
    "description": "Description: The `iris/test_plugins.py` module is a testing script for various data intake plugins, verifying connectivity and basic data fetching capabilities, and generating a JSON report of results.",
    "issues": []
  },
  "iris/test_reddit.py": {
    "description": "Description: The `iris/test_reddit.py` module is a basic test script for the Reddit plugin, verifying API connectivity and data fetching.",
    "issues": []
  },
  "iris/test_reddit_direct.py": {
    "description": "Description: The `iris/test_reddit_direct.py` module is a manual test script for the Reddit plugin, requiring direct input of API credentials.",
    "issues": []
  },
  "iris/test_world_bank.py": {
    "description": "Description: The `iris/test_world_bank.py` module is a basic test script for the World Bank plugin, verifying API connectivity and data fetching.",
    "issues": []
  },
  "iris/variable_ingestion.py": {
    "description": "Description: The `iris/variable_ingestion.py` module fetches a small set of live financial/economic variables from FRED and Yahoo Finance and registers them with the `core.variable_registry`.",
    "issues": []
  },
  "iris/variable_recommender.py": {
    "description": "Description: The `iris/variable_recommender.py` module is a functional CLI tool for recommending variables based on performance logs and registering them with PulseGrow.",
    "issues": []
  },
  "learning/diagnose_pulse.py": {
    "description": "Description: shows it's a functional diagnostic script for summarizing symbolic overlays, fragility, and capital exposure, with potential for enhanced logging and error handling.",
    "issues": []
  },
  "learning/engines/active_experimentation.py": {
    "description": "Description: Stub module for active experimentation; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement experiment logic (File: learning/engines/active_experimentation.py)"
    ]
  },
  "learning/engines/anomaly_remediation.py": {
    "description": "Description: Stub module for anomaly detection and remediation; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement anomaly detection logic (File: learning/engines/anomaly_remediation.py)"
    ]
  },
  "learning/engines/audit_reporting.py": {
    "description": "This module is a stub module for audit reporting; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement report generation logic (File: learning/engines/audit_reporting.py)"
    ]
  },
  "learning/engines/causal_inference.py": {
    "description": "Description: Stub module for causal inference; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement causal inference logic (File: learning/engines/causal_inference.py)"
    ]
  },
  "learning/engines/continuous_learning.py": {
    "description": "Description: Stub module for continuous learning; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement trust weight update logic (File: learning/engines/continuous_learning.py)"
    ]
  },
  "learning/engines/external_integration.py": {
    "description": "Description: Stub module for external data/model integration; core logic unimplemented, no tests.",
    "issues": [
      "TODO (Line 20): # TODO: Implement data ingestion logic (File: learning/engines/external_integration.py)"
    ]
  },
  "learning/engines/feature_discovery.py": {
    "description": "This module discovers features using clustering/selection; functional but with unimplemented areas and hardcoding.",
    "issues": []
  },
  "learning/forecast_pipeline_runner.py": {
    "description": "Orchestrates the Pulse forecast processing pipeline, including trust scoring, contradiction detection, Epistemic Mirror integration, compression, and digest generation. Key Gap: Full `WorldState` integration is pending.",
    "issues": []
  },
  "learning/history_tracker.py": {
    "description": "Description: shows a functional module for logging variable evolution during simulations to JSONL files, with potential improvements in error handling and data consumption tooling.",
    "issues": []
  },
  "learning/learning.py": {
    "description": "Core meta-learning engine for Pulse, responsible for adapting system performance by analyzing simulation lineage, updating trust scores, managing symbolic overlays, and integrating specialized learning sub-engines. Key Gap: Several sub-engines are placeholders.",
    "issues": []
  },
  "learning/learning_profile.py": {
    "description": "Defines the `LearningProfile` dataclass for storing performance metrics and statistics related to various learning processes (symbolic, statistical, causal). Key Gap: Designed for extension; currently minimal.",
    "issues": []
  },
  "learning/output_data_reader.py": {
    "description": "Provides a unified interface to read and parse Pulse output files (forecasts, logs, etc.) and external data into Pandas DataFrames for analysis. Key Gap: Hardcoded paths and basic error handling.",
    "issues": []
  },
  "learning/plia_stub.py": {
    "description": "Performs a \"stub\" Pulse Logic Integrity Audit (PLIA), checking coherence between symbolic tension in `WorldState` and total deployed capital. Key Gap: \"Stub\" implies a more comprehensive PLIA is planned.",
    "issues": []
  },
  "learning/promote_memory_forecasts.py": {
    "description": "CLI tool to select \"promotable\" forecasts from a batch and export them to core forecast memory. Relies on `analytics.forecast_memory_promoter`. Key Gap: Core promotion logic is external.",
    "issues": []
  },
  "learning/pulse_ui_audit_cycle.py": {
    "description": "Description: CLI tool for comparing recursive forecast cycles and generating improvement audit reports, relying on `analytics.recursion_audit` for core logic.",
    "issues": []
  },
  "learning/recursion_audit.py": {
    "description": "Description: Compares forecast batches from recursive cycles to summarize Pulse's improvement trajectory by analyzing confidence, trust labels, retrodiction error, and symbolic arc shifts.",
    "issues": []
  },
  "learning/retrodiction_bootstrap.py": {
    "description": "Description: Starter pipeline to gather and normalize diverse data for Pulse retrodiction, currently functional for economic/market data but with significant placeholder sections for other data categories and requiring API key setup.",
    "issues": []
  },
  "learning/retrodiction_curriculum.py": {
    "description": "Description: Module for managing historical simulation batches for learning, currently in an early stage with mocked core functionalities and placeholder imports.",
    "issues": []
  },
  "learning/symbolic_sweep_scheduler.py": {
    "description": "Description: Periodically re-processes blocked forecasts to recover them, logs results, and lacks automated scheduling and unit tests.",
    "issues": []
  },
  "learning/trace_forecast_episode.py": {
    "description": "Description: CLI tool for tracing forecast episode lineage and summarizing symbolic drift, relying on `analytics.forecast_episode_tracer` for core logic.",
    "issues": []
  },
  "learning/transforms/basic_transforms.py": {
    "description": "Description: Provides several basic feature transformation functions for pandas DataFrames, with `sentiment_score` being a notable placeholder and a general lack of unit tests.",
    "issues": []
  },
  "learning/transforms/data_pipeline.py": {
    "description": "This module provides basic data preprocessing (imputation, normalization, top-k variance selection); lacks tests.",
    "issues": []
  },
  "learning/transforms/rolling_features.py": {
    "description": "Description: Analysis of [`learning/transforms/rolling_features.py`](learning/transforms/rolling_features.py:1) shows a module for calculating rolling mean features, which is functional for its limited scope and has good tests for the existing function, but could be expanded with more rolling window operations and edge case testing.",
    "issues": []
  },
  "learning/trust_audit.py": {
    "description": "Description: Analysis of [`learning/trust_audit.py`](learning/trust_audit.py:1) shows it provides a strategic summary of recent foresight memory, including trust band counts, metric averages, and integrity checks, with a notable unimplemented [`audit_trust()`](learning/trust_audit.py:58) function.",
    "issues": []
  },
  "main.py": {
    "description": "Main entry point for the Pulse application, likely orchestrating core functionalities",
    "issues": []
  },
  "memory/cluster_mutation_tracker.py": {
    "description": "Description: Analysis of [`memory/cluster_mutation_tracker.py`](memory/cluster_mutation_tracker.py:1) shows it identifies the most evolved forecast in each symbolic cluster by mutation depth, is largely complete with basic tests, but lacks dedicated unit tests and could enhance evolution metrics.",
    "issues": []
  },
  "memory/contradiction_resolution_tracker.py": {
    "description": "Description: Analysis of [`memory/contradiction_resolution_tracker.py`](memory/contradiction_resolution_tracker.py:1) shows it tracks forecast contradiction statuses, logs outcomes, and summarizes results; it's functional but lacks dedicated tests and has some hardcoding.",
    "issues": []
  },
  "memory/forecast_episode_tracer.py": {
    "description": "Description: Analysis of [`memory/forecast_episode_tracer.py`](memory/forecast_episode_tracer.py:1) shows it tracks forecast lineage and symbolic mutations, is functionally complete for its scope, but lacks dedicated tests and has minor hardcoding.",
    "issues": []
  },
  "memory/forecast_memory.py": {
    "description": "This module manages forecast storage, retrieval, persistence, and pruning; largely complete with some refinement areas.",
    "issues": []
  },
  "memory/forecast_memory_entropy.py": {
    "description": "Description: Analysis of [`memory/forecast_memory_entropy.py`](../memory/forecast_memory_entropy.py) shows a module for measuring symbolic entropy and novelty in forecasts, detecting stagnation and redundancy. It is functionally complete with internal tests but lacks dedicated external test files and could be extended with more sophisticated metrics.",
    "issues": []
  },
  "memory/memory_repair_queue.py": {
    "description": "Description: Analysis of [`memory/memory_repair_queue.py`](../memory/memory_repair_queue.py) shows it re-evaluates discarded forecasts for possible recovery after criteria changes, is functionally complete but lacks tests and has some hardcoding.",
    "issues": []
  },
  "memory/pulse_memory_audit_report.py": {
    "description": "Description: The primary role of the [`memory/pulse_memory_audit_report.py`](memory/pulse_memory_audit_report.py:1) module is to provide a basic audit of the `ForecastMemory` object. It reports on the total number of forecasts stored, the domains covered by these forecasts, and can optionally export a CSV file containing details like forecast ID, domain, and confidence for each stored forecast.",
    "issues": []
  },
  "memory/pulse_memory_guardian.py": {
    "description": "Description: The primary role of the [`memory/pulse_memory_guardian.py`](memory/pulse_memory_guardian.py:1) module is to manage the lifecycle of forecast memory. This includes responsibilities such as pruning older or less relevant forecasts, archiving variable data (\"fossils\"), managing the status of variables (soft retirement, reconsideration), and ensuring the coherence of forecasts stored in memory.",
    "issues": []
  },
  "memory/pulsegrow.py": {
    "description": "Description: The `memory/pulsegrow.py` module, named \"PulseGrow \u2014 Variable Evolution Engine,\" is designed to introduce new candidate variables into Pulse simulations. Its primary role is to evaluate the performance of these candidate variables over time and selectively promote them to the core memory if they demonstrate an ability to enhance forecast clarity, symbolic richness, or capital foresight. Key functionalities include candidate registration, performance scoring, symbolic correlation testing, and a gated promotion process with an audit trail.",
    "issues": []
  },
  "memory/rule_cluster_engine.py": {
    "description": "Description: Analysis of [`memory/rule_cluster_engine.py`](memory/rule_cluster_engine.py:1) shows it clusters and scores simulation rules based on domain and mutation volatility from logs, aiding meta-learning. It's functional but lacks dedicated tests and has some hardcoded default paths.",
    "issues": []
  },
  "memory/trace_audit_engine.py": {
    "description": "Description: Manages and audits simulation traces, including ID generation, storage, replay, and registration to forecast memory.",
    "issues": []
  },
  "memory/trace_memory.py": {
    "description": "Description: Analysis of [`memory/trace_memory.py`](../memory/trace_memory.py) shows a module for logging and querying simulation trace metadata, which is functionally complete but could benefit from scalability improvements and dedicated tests.",
    "issues": []
  },
  "memory/variable_cluster_engine.py": {
    "description": "Description: Analysis of [`memory/variable_cluster_engine.py`](../memory/variable_cluster_engine.py) shows it clusters simulation variables by domain and tag, calculates volatility scores, and provides summaries; it's functional with inline tests but could benefit from dedicated tests, correlation-based clustering, and more advanced volatility metrics.",
    "issues": []
  },
  "memory/variable_performance_tracker.py": {
    "description": "Description: Analysis of [`memory/variable_performance_tracker.py`](../memory/variable_performance_tracker.py) shows a module for tracking variable impact on simulations, logging contributions, scoring trust/fragility, and exporting data; it is largely complete but lacks dedicated tests and could benefit from more advanced drift analysis.",
    "issues": []
  },
  "mlflow_tracking_example.py": {
    "description": "Minimal, standalone script demonstrating basic MLflow logging",
    "issues": []
  },
  "myapp/alembic/env.py": {
    "description": "This module is a standard Alembic environment script for DB migrations using SQLModel; appears complete.",
    "issues": []
  },
  "operator_interface/learning_log_viewer.py": {
    "description": "This module provides CLI tools to load, filter, summarize, and render Pulse learning log events, and display variable/rule trust scores; console-focused with potential for UI integration.",
    "issues": []
  },
  "operator_interface/mutation_digest_exporter.py": {
    "description": "This module exports a unified markdown digest of rule cluster volatility, variable cluster instability, and learning/mutation events.",
    "issues": []
  },
  "operator_interface/mutation_log_viewer.py": {
    "description": "Description: Provides CLI tools to view summaries of learning and rule mutation logs, aiding in diagnostics and trust evolution audits.",
    "issues": []
  },
  "operator_interface/operator_brief_generator.py": {
    "description": "This module generates a Markdown summary of recent simulation forecasts, including alignment scores, symbolic arcs, tags, and risk notes.",
    "issues": []
  },
  "operator_interface/rule_cluster_digest_formatter.py": {
    "description": "Description: Analysis of [`operator_interface/rule_cluster_digest_formatter.py`](operator_interface/rule_cluster_digest_formatter.py:1) shows it formats rule cluster summaries into Markdown, highlighting volatility, and exports them; it's functional but lacks dedicated tests and has some hardcoded values.",
    "issues": []
  },
  "operator_interface/rule_cluster_viewer.py": {
    "description": "Description: Analysis of [`operator_interface/rule_cluster_viewer.py`](operator_interface/rule_cluster_viewer.py:1) shows it displays rule clusters by domain and volatility for operator review, relying on `analytics.rule_cluster_engine` and lacking dedicated tests.",
    "issues": []
  },
  "operator_interface/strategos_digest.py": {
    "description": "Description: The `operator_interface/strategos_digest.py` module generates a comprehensive Markdown \"Strategos Digest\" summarizing recent forecasts, trust metrics, symbolic learning, drift analysis, and visualizations.",
    "issues": [
      "TODO (Line 139): # TODO: Implement actual pattern detection (File: operator_interface/strategos_digest.py)",
      "TODO (Line 146): # TODO: Implement actual loop detection (File: operator_interface/strategos_digest.py)"
    ]
  },
  "operator_interface/symbolic_contradiction_digest.py": {
    "description": "Description: Processes symbolic contradiction cluster events from logs into a Markdown summary, appearing complete for its defined scope.",
    "issues": []
  },
  "operator_interface/symbolic_revision_report.py": {
    "description": "This module generates a Markdown summary of symbolic tuning results, detailing license changes, alignment score improvements, and applied revision plans.",
    "issues": []
  },
  "operator_interface/variable_cluster_digest_formatter.py": {
    "description": "Description: Analysis of [`operator_interface/variable_cluster_digest_formatter.py`](operator_interface/variable_cluster_digest_formatter.py:1) shows it formats variable cluster summaries into Markdown, highlighting volatility, and exports them; it's functional but lacks dedicated tests and has some hardcoded values.",
    "issues": []
  },
  "patch_imports.py": {
    "description": "Utility to patch Python import statements, likely for refactoring or testing",
    "issues": []
  },
  "phantom_function_scanner.py": {
    "description": "Analyzes Python code to find function calls that lack local definitions, aiding in code cleanup and error prevention",
    "issues": []
  },
  "pipeline/evaluator.py": {
    "description": "Compares Pulse/GPT forecasts, proposes rule changes. It has TODOs.",
    "issues": [
      "TODO (Line 20): # TODO: set up simulation_client or other services (File: pipeline/evaluator.py)",
      "TODO (Line 44): # TODO: Replace with actual data loading mechanism if different (File: pipeline/evaluator.py)",
      "TODO (Line 86): # TODO: Implement more sophisticated feedback generation and storage (File: pipeline/evaluator.py)",
      "TODO (Line 88): # TODO: Integrate proposed_rule_changes into a management mechanism (File: pipeline/evaluator.py)",
      "TODO (Line 110): # TODO: implement evaluation logic calling engine.simulate_forward (File: pipeline/evaluator.py)",
      "TODO (Line 111): # TODO: Incorporate historical forecast data for evaluation and training (File: pipeline/evaluator.py)",
      "TODO (Line 209): # TODO: Implement more sophisticated rule generation/pruning logic (File: pipeline/evaluator.py)"
    ]
  },
  "pipeline/gpt_caller.py": {
    "description": "Description: Provides a Python class to interact with OpenAI's GPT models.",
    "issues": []
  },
  "pipeline/ingestion_service.py": {
    "description": "Description: Provides an OO wrapper for IrisScraper ingestion, allowing importable and CLI execution; complete for its scope.",
    "issues": []
  },
  "pipeline/model_manager.py": {
    "description": "This module is a skeleton module for model training and registry interaction. It has many TODOs and no tests.",
    "issues": [
      "TODO (Line 15): # TODO: set up client/connection here (File: pipeline/model_manager.py)",
      "TODO (Line 23): # TODO: implement training or fine-tuning logic (File: pipeline/model_manager.py)",
      "TODO (Line 35): # TODO: implement metrics logging (e.g., MLflow.log_metrics) (File: pipeline/model_manager.py)"
    ]
  },
  "pipeline/orchestrator.py": {
    "description": "Schedules and runs the AI training cycle. It is functional but needs error handling, advanced scheduling, and tests.",
    "issues": []
  },
  "pipeline/preprocessor.py": {
    "description": "Description: Analysis of `pipeline/preprocessor.py` reveals a skeleton module for data preprocessing (loading, merging, normalizing, feature computation) with most core methods unimplemented (marked `TODO`) and no existing tests; only basic feature saving to Parquet is partially implemented.",
    "issues": [
      "TODO (Line 17): # TODO: implement loading logic for PFPA archive, retrodiction memory, and IRIS snapshots (File: pipeline/preprocessor.py)",
      "TODO (Line 24): # TODO: implement merge logic using self.raw_data (File: pipeline/preprocessor.py)",
      "TODO (Line 31): # TODO: apply scaling/normalization to self.features (File: pipeline/preprocessor.py)",
      "TODO (Line 38): # TODO: implement feature engineering steps (File: pipeline/preprocessor.py)",
      "TODO (Line 45): # TODO: persist self.features to disk (e.g., CSV, Parquet) (File: pipeline/preprocessor.py)"
    ]
  },
  "pipeline/rule_applier.py": {
    "description": "Description: This module loads proposed rule changes from JSON and applies them to an active rule set; it's functional for its scope but lacks persistence for updated rules and advanced rule application logic.",
    "issues": []
  },
  "pipeline/rule_engine.py": {
    "description": "Description: Analysis of `pipeline/rule_engine.py` reveals it is a stub module intended to generate, evaluate, and prune dynamic rules using GPT and symbolic systems, with all core logic currently unimplemented via `TODO` comments and no existing tests.",
    "issues": [
      "TODO (Line 14): # TODO: set up GPT client and symbolic_system interfaces (File: pipeline/rule_engine.py)",
      "TODO (Line 20): # TODO: implement rule generation logic using GPT and pulse_symbolic_revision_planner (File: pipeline/rule_engine.py)",
      "TODO (Line 27): # TODO: implement batch rule evaluation in engine.rule_mutation_engine (File: pipeline/rule_engine.py)",
      "TODO (Line 34): # TODO: implement pruning strategy (File: pipeline/rule_engine.py)"
    ]
  },
  "pulse_desktop/tkinter_ui.py": {
    "description": "Provides a comprehensive Tkinter GUI for Pulse system interaction, with many features but some placeholder visualizations and export functions.",
    "issues": []
  },
  "pulse_desktop/ui_operator.py": {
    "description": "Provides a CLI and basic console for inspecting Pulse's recursive intelligence, including recursion comparison, variable plotting, and report generation.",
    "issues": []
  },
  "pulse_desktop/ui_shell.py": {
    "description": "Provides a CLI for core simulation, forecast replay, batch runs, tests, and CLI hooks.",
    "issues": []
  },
  "recursive_training/advanced_metrics/retrodiction_curriculum.py": {
    "description": "This module dynamically selects training data based on model uncertainty and performance.",
    "issues": []
  },
  "recursive_training/config/default_config.py": {
    "description": "Provides default dataclass-based configurations for recursive training. Its `update_config` method is a placeholder.",
    "issues": []
  },
  "recursive_training/data/advanced_feature_processor.py": {
    "description": "Description: The primary role of this module is to serve as an integration point for applying various advanced data processing techniques (time-frequency decomposition, graph-based features, self-supervised learning) to economic time series data and integrate them into data pipelines. Main Gap: Its functionality is highly dependent on the completeness and correctness of the external modules it delegates its core processing logic to.",
    "issues": []
  },
  "recursive_training/data/data_store.py": {
    "description": "Description: This module provides a unified and robust system for storing and retrieving data within the recursive training framework, featuring data versioning, compression, indexing, querying, dataset management, and cleanup policies. Main Gap: The \\\"Hybrid storage approach\\\" mentioned in the docstring for optimizing different data types is not fully detailed in the implementation, which primarily shows a file-system based approach.",
    "issues": []
  },
  "recursive_training/data/feature_processor.py": {
    "description": "Handles comprehensive feature extraction, transformation, and preparation for recursive training.",
    "issues": []
  },
  "recursive_training/data/feature_processor_integration.py": {
    "description": "Description: This module serves as an integration layer, enhancing a standard feature processor by incorporating advanced data processing techniques like time-frequency decomposition, graph-based features, and self-supervised representation learning. Main Gap: The module has a placeholder for additional transformations specific to advanced features, and methods like `get_feature_names()` and `get_feature_importance()` might not accurately reflect the complete set of features if advanced features are not integrated.",
    "issues": []
  },
  "recursive_training/data/graph_based_features.py": {
    "description": "Description: This module constructs co-movement graphs from time series data and extracts various graph-based metrics (centrality, clustering, communities, etc.) for use in retrodiction processes. Main Gap: The module lacks specific unit tests for the `GraphFeatureExtractor` class and its methods, and the current rolling window correlation logic averages correlations rather than using more advanced temporal graph analysis.",
    "issues": []
  },
  "recursive_training/data/ingestion_manager.py": {
    "description": "Description: This module manages the ingestion of data from diverse sources (files, APIs, databases) into the recursive training system, including validation, transformation (lightly implemented), storage via `RecursiveDataStore`, and placeholder cost control. Main Gap: Advanced cost control, rate limiting, comprehensive schema validation beyond `ForecastRecord`, data transformation/preprocessing, caching, and parallel ingestion are either placeholders or not fully implemented.",
    "issues": []
  },
  "recursive_training/data/optimized_data_store.py": {
    "description": "Description: The `OptimizedDataStore` module provides an enhanced version of the `RecursiveDataStore`, offering optimized data storage and retrieval mechanisms (vectorized operations, memory-mapping, batch retrieval, LRU caching, Parquet/HDF5 support) for improved performance with large datasets, especially time-series data, while maintaining compatibility with the base store. Main Gap: Enhanced error reporting (avoiding direct prints, using custom exceptions), more robust configuration with schema validation, and addressing potential security risks with the `pickle` fallback are key areas for improvement.",
    "issues": []
  },
  "recursive_training/data/s3_data_store.py": {
    "description": "Description: The `S3DataStore` module extends `StreamingDataStore` to provide an enhanced data storage solution with AWS S3 integration, allowing loading, streaming, and caching of data (Parquet, HDF5, Pickle) from S3 buckets for efficient data transfer and management. Main Gap: Key areas for improvement include more robust error handling/retries for S3 operations, support for advanced S3 features (e.g., versioning, lifecycle policies), enhanced security considerations (IAM roles, encryption), and a more sophisticated cache invalidation strategy beyond simple time-based expiry.",
    "issues": []
  },
  "recursive_training/data/self_supervised_learning.py": {
    "description": "Provides a framework for self-supervised representation learning from time series data using autoencoder architectures.",
    "issues": [
      "NotImplementedError (Line 53): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)",
      "NotImplementedError (Line 57): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)",
      "NotImplementedError (Line 61): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)",
      "NotImplementedError (Line 65): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)",
      "NotImplementedError (Line 69): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)",
      "NotImplementedError (Line 73): raise NotImplementedError (File: recursive_training/data/self_supervised_learning.py)"
    ]
  },
  "recursive_training/data/streaming_data_store.py": {
    "description": "Description: The `StreamingDataStore` module extends `OptimizedDataStore` to provide advanced data storage and retrieval with a strong emphasis on streaming capabilities for large datasets, featuring integration with Apache Arrow/Parquet, prefetching, caching, and progressive loading to reduce memory footprint and improve performance. Main Gap: Key areas for improvement include implementing more sophisticated predictive loading beyond the current basic heuristic, potentially integrating PyArrow Flight for optimized data transfer, and making Parquet compression methods configurable.",
    "issues": []
  },
  "recursive_training/data/time_frequency_decomposition.py": {
    "description": "Description: The module provides time-frequency decomposition (STFT, CWT, DWT), feature extraction, and regime shift detection for time series data, with graceful handling of optional dependencies.",
    "issues": []
  },
  "recursive_training/error_handling/error_handler.py": {
    "description": "Description: The `RecursiveTrainingErrorHandler` module provides a centralized mechanism for logging, alerting, and attempting basic recovery from errors encountered during the recursive AI training process.",
    "issues": []
  },
  "recursive_training/error_handling/recovery.py": {
    "description": "This module provides a foundational error recovery framework. Its core logic is a placeholder.",
    "issues": []
  },
  "recursive_training/error_handling/training_monitor.py": {
    "description": "Monitors training runs for errors, metric anomalies, and threshold violations, providing alerts.",
    "issues": []
  },
  "recursive_training/integration/config_manager.py": {
    "description": "Description: Analysis of `recursive_training/integration/config_manager.py` shows it manages loading, saving, and accessing recursive learning configurations from a JSON file; it is thread-safe and provides defaults.",
    "issues": []
  },
  "recursive_training/integration/cost_controller.py": {
    "description": "Description: The `CostController` module provides a central service for monitoring, tracking, and limiting API/token usage costs within the Recursive Training System, enforcing budget thresholds and offering rate limiting.",
    "issues": []
  },
  "recursive_training/integration/process_registry.py": {
    "description": "Provides a thread-safe registry for managing and tracking active recursive learning processes.",
    "issues": []
  },
  "recursive_training/integration/pulse_adapter.py": {
    "description": "Description: Provides an integration layer between the Recursive Training System and Pulse's core components, handling communication and data conversion.",
    "issues": []
  },
  "recursive_training/metrics/async_metrics_collector.py": {
    "description": "Description: Provides a non-blocking, queued system for collecting and storing metrics asynchronously.",
    "issues": []
  },
  "recursive_training/metrics/bayesian_adapter.py": {
    "description": "Description: Integrates recursive training metrics with Pulse's Bayesian trust system, with a fallback mechanism.",
    "issues": []
  },
  "recursive_training/metrics/metrics_store.py": {
    "description": "Provides centralized storage, retrieval, and querying of training metrics and operational costs.",
    "issues": []
  },
  "recursive_training/metrics/training_metrics.py": {
    "description": "Description: Implements core metrics calculation, tracking, and performance insights for recursive training.",
    "issues": []
  },
  "recursive_training/parallel_trainer.py": {
    "description": "Description: A Dask-based parallel training framework for retrodiction.",
    "issues": []
  },
  "recursive_training/regime_sensor/event_stream_manager.py": {
    "description": "Description: Manages real-time external event streams for regime shift detection.",
    "issues": []
  },
  "recursive_training/regime_sensor/integration.py": {
    "description": "Description: Integrates regime sensing with retrodiction and counterfactual simulation, demonstrating component interplay.",
    "issues": []
  },
  "recursive_training/regime_sensor/regime_detector.py": {
    "description": "Identifies market/economic regime shifts from event streams and market data. Its core detection logic is a placeholder.",
    "issues": []
  },
  "recursive_training/regime_sensor/retrodiction_trigger.py": {
    "description": "Description: Connects regime change events to retrodiction model re-evaluation via snapshots.",
    "issues": []
  },
  "recursive_training/rules/hybrid_adapter.py": {
    "description": "Provides a flexible adapter for converting between dictionary-based and object-oriented rule representations.",
    "issues": []
  },
  "recursive_training/rules/rule_evaluator.py": {
    "description": "Description: Evaluates rule effectiveness, performance, and gathers metrics for rule refinement.",
    "issues": []
  },
  "recursive_training/rules/rule_generator.py": {
    "description": "Description: Generates rules using a GPT-Symbolic feedback loop; core generation/refinement logic is placeholder.",
    "issues": []
  },
  "recursive_training/rules/rule_repository.py": {
    "description": "Description: A comprehensive system for rule persistence, versioning, and lifecycle management.",
    "issues": []
  },
  "recursive_training/run_training.py": {
    "description": "Description: Orchestrates Pulse retrodiction training, especially for AWS Batch, handling configuration, setup, execution, and S3 results upload.",
    "issues": []
  },
  "runversionone.py": {
    "description": "Legacy script for baseline retrodiction and parallel training comparison",
    "issues": []
  },
  "scripts/analysis/analyze_historical_data_quality.py": {
    "description": "Description: Analyzes historical timeline data quality for retrodiction training, checking completeness, depth, missing values, temporal alignment, and generates an HTML report. Major Gap: Imputation Effectiveness Analysis: While mentioned in the docstring, the script primarily checks for missing values rather than evaluating the effectiveness of prior imputation.",
    "issues": []
  },
  "scripts/benchmarking/benchmark_retrodiction.py": {
    "description": "Description: The primary role of `benchmark_retrodiction.py` is to benchmark the end-to-end retrodiction training process within the Pulse system. It aims to measure the performance of key components involved in this process, including data loading, causal discovery, trust updates, and curriculum selection. Major Gap: Integrated Full Training Benchmark: While the current full training benchmark uses mocks, an extension could allow for a more integrated (less mocked) run to capture performance characteristics of actual component interactions, though this would be more complex to set up and control.",
    "issues": []
  },
  "scripts/benchmarking/check_benchmark_deps.py": {
    "description": "Description: The primary role of this module is to verify that all necessary Python packages and internal project modules required for running the retrodiction benchmarking process are installed and available in the current environment. Major Gap: No major implementation gaps identified; the module is focused on its specific dependency checking goal.",
    "issues": []
  },
  "scripts/data_management/improve_historical_data.py": {
    "description": "Description: The primary purpose of the `improve_historical_data.py` module is to enhance historical timeline data, specifically for use in retrodiction training by modifying an ingestion plugin, cleaning data, and saving outputs. Major Gap: Robust Plugin Modification: The module uses direct string replacement to alter another Python file (`historical_ingestion_plugin.py`), which is highly fragile.",
    "issues": []
  },
  "scripts/legacy/runversionone.py": {
    "description": "Description: The module serves as an executable script to establish a baseline forecast accuracy using retrodiction and execute an optimized, parallel retrodiction training process, comparing their performances. Major Gap: Robustness of `LearningEngine` Interaction: The script makes assumptions about how `LearningEngine` and `EnhancedRecursiveTrainingMetrics` interact, particularly around lines 168-178. This interaction point might be fragile or could benefit from a more explicit interface or clarification in the involved modules.",
    "issues": []
  },
  "scripts/reporting/api_key_report.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "scripts/build_symbol_index.py": {
    "description": "Builds a JSON and ChromaDB vector index of Python symbols in the codebase.",
    "issues": []
  },
  "scripts/capture_missing_variables.py": {
    "description": "Description: Fetches, processes, and stores economic data from Alpha Vantage.",
    "issues": []
  },
  "scripts/discover_available_variables.py": {
    "description": "Description: Discovers variables from ingestion plugins by inspecting plugin code and a variable catalog, then generates a Markdown report.",
    "issues": []
  },
  "scripts/run_causal_benchmarks.py": {
    "description": "Description: A CLI tool for executing causal-only benchmarks by running simulations with gravity disabled, offering programmatic or subprocess execution and scenario configuration.",
    "issues": []
  },
  "simulation_engine/batch_runner.py": {
    "description": "Description: Orchestrates batch simulation runs, including configuration loading, simulation execution, forecast generation, and forecast pipeline processing.",
    "issues": [
      "TODO (Line 265): # TODO: Add CLI/config file support for batch execution. (File: simulation_engine/batch_runner.py)"
    ]
  },
  "simulation_engine/causal_rules.py": {
    "description": "Description: Defines and applies causal rules for the simulation, modulating their effects based on a Bayesian trust mechanism and providing rule performance statistics.",
    "issues": []
  },
  "simulation_engine/decay_logic.py": {
    "description": "Defines and applies linear decay to simulation state, with planned extensions. It lacks tests.",
    "issues": []
  },
  "simulation_engine/historical_retrodiction_runner.py": {
    "description": "Description: This module is a compatibility layer for deprecated functionality, now part of simulation_engine/simulator_core.py, and exists solely to maintain backward compatibility with existing tests.",
    "issues": []
  },
  "simulation_engine/pulse_signal_router.py": {
    "description": "Description: The module routes external narrative signals (e.g., 'ai_panic') to modify the simulation's WorldState through predefined, hardcoded handlers and numerical adjustments.",
    "issues": []
  },
  "simulation_engine/rl_env.py": {
    "description": "Description: Defines an OpenAI Gym-style environment for RL-based rule adaptation, largely complete but could benefit from more sophisticated reward/observation spaces and dedicated tests.",
    "issues": []
  },
  "simulation_engine/rule_engine.py": {
    "description": "Description: Executes static causal rules against the WorldState, applies their effects if conditions are met, and generates an audit trail of triggered rules; key gaps include lack of dynamic rule loading and dedicated unit tests.",
    "issues": []
  },
  "simulation_engine/rule_mutation_engine.py": {
    "description": "Proposes and logs mutations to rule thresholds based on volatility. It does not yet save changes or implement broader mutation types/triggers.",
    "issues": []
  },
  "simulation_engine/simulate_backward.py": {
    "description": "Description: The module performs backward simulation (retrodiction) by loading a historical `WorldState` snapshot and stepping backward in time, inverting a decay process. Major Gap: Retrodiction Score Calculation: The most significant gap is the lack of an actual implementation for the `retrodiction_score`. The comment \"implement actual logic\" indicates this is a known TODO. This score is crucial for evaluating the accuracy of the retrodiction process.",
    "issues": []
  },
  "simulation_engine/simulation_drift_detector.py": {
    "description": "Description: The module compares internal simulation artifacts (rule activations, overlay trajectories, simulation structure) from two different simulation runs to identify and quantify \"drift,\" serving as a command-line tool or importable library. Major Gap: Automated Testing: The existing test function, `_test_drift_detector()`, is for manual validation and not executed by default. Integrating this or more comprehensive tests into an automated testing framework (e.g., pytest) is a clear next step.",
    "issues": []
  },
  "simulation_engine/simulator_core.py": {
    "description": "Description: The `simulation_engine/simulator_core.py` module is the central simulation engine for Pulse, managing turn-by-turn forward simulations based on a `WorldState`. It handles state changes via decay and rule execution, integrates with symbolic logic, trust systems, and a learning engine, and supports counterfactual simulations and historical trace validation. Major Gap: The most significant gap is the unimplemented `simulate_backward()` function, which is currently a placeholder, preventing true reverse simulation capabilities. Additionally, several planned features like batch/parallel simulation, checkpointing, and comprehensive unit tests are listed as TODOs.",
    "issues": [
      "TODO (Line 31): TODO: (File: simulation_engine/simulator_core.py)",
      "NotImplementedError (Line 626): raise NotImplementedError(\"Parallel execution is not yet supported\") (File: simulation_engine/simulator_core.py)"
    ]
  },
  "simulation_engine/state_mutation.py": {
    "description": "Description: The `simulation_engine/state_mutation.py` module manages controlled changes to the `WorldState` object, handling updates for numeric variables, symbolic overlays (emotional states), and capital exposure. It ensures validated, bounded, and logged mutations, supporting inputs from rules, decay, and signals. Major Gap: The module is largely complete for its current scope. Potential gaps are more about future extensibility: needing new functions if new `WorldState` data categories arise, implementing more complex validation logic beyond simple bounds, and making hardcoded default values (like decay rates or variable bounds) configurable.",
    "issues": []
  },
  "simulation_engine/train_rl_agent.py": {
    "description": "Description: The `simulation_engine/train_rl_agent.py` module trains a Reinforcement Learning (RL) agent using the PPO algorithm from `stable-baselines3` within a custom `SimulationEnv`. It handles the training loop, model saving, basic evaluation, and integrates with `mlflow` for experiment tracking. Major Gap: While functional for basic training, the module lacks hyperparameter tuning, advanced evaluation strategies, curriculum learning, and mechanisms for agent deployment/inference. Key parameters are also hardcoded or passed as default arguments rather than managed via configuration files.",
    "issues": []
  },
  "simulation_engine/turn_engine.py": {
    "description": "Description: The module `simulation_engine/turn_engine.py` controls the simulation loop at the individual turn level, applying symbolic decay, executing causal rules, running an auditable rule engine, allowing custom rule injection, incrementing the turn counter, and returning an audit trail. Major Gap: The most significant gap is the commented-out license enforcement logic (`simulation_engine/turn_engine.py:114-119`).",
    "issues": []
  },
  "simulation_engine/worldstate_monitor.py": {
    "description": "Description: The module `simulation_engine/worldstate_monitor.py` provides functionalities for displaying and logging various aspects of the Pulse simulation's `WorldState`, including symbolic overlays, capital exposure, variable states, and changes in overlays. It also integrates with a `gravity_explainer` component and can simulate running a batch of forecasts. Major Gap: The `run_batch_forecasts` function uses example metadata (`metadata = {\"confidence\": 0.7}`) and the `symbolic_block` parameter lacks a concrete implementation or example, suggesting these are areas for future extension.",
    "issues": []
  },
  "simulation_engine/rules/pulse_rule_expander.py": {
    "description": "Description: Generates candidate new rules by analyzing regret chains, with CLI support and plans for analyzing forecast arc shifts and symbolic deltas.",
    "issues": []
  },
  "simulation_engine/rules/pulse_rule_explainer.py": {
    "description": "Explains forecast contributions by matching to rule fingerprints; CLI for batch processing.",
    "issues": []
  },
  "simulation_engine/rules/reverse_rule_engine.py": {
    "description": "Traces causal rule chains from observed deltas using fingerprints; supports fuzzy matching and suggests new rules.",
    "issues": []
  },
  "simulation_engine/rules/reverse_rule_mapper.py": {
    "description": "This module maps observed state changes to candidate rules using fingerprints and logic from `rule_matching_utils`, and can validate fingerprint schemas.",
    "issues": []
  },
  "simulation_engine/rules/rule_audit_layer.py": {
    "description": "Description: Logs detailed audit traces for rule executions, capturing metadata and state changes; complete for its scope but lacks data persistence and dedicated tests.",
    "issues": []
  },
  "simulation_engine/rules/rule_autoevolver.py": {
    "description": "Description: This module provides meta-learning for rule evaluation, mutation, and lifecycle management, with a CLI, but has placeholder scoring logic and lacks an automated evolution loop.",
    "issues": []
  },
  "simulation_engine/rules/rule_coherence_checker.py": {
    "description": "Description: Scans rule fingerprints for logical, structural, and schema errors.",
    "issues": []
  },
  "simulation_engine/rules/rule_fingerprint_expander.py": {
    "description": "Description: This module suggests and validates new rule fingerprints derived from observed system state deltas or forecast batches, considering confidence levels. It includes a CLI for these operations and a stub for a future rule approval workflow. Main Gap: The most significant gap is the unimplemented rule approval workflow ([`submit_rule_for_approval()`](simulation_engine/rules/rule_fingerprint_expander.py:81)). Additionally, more robust data input mechanisms beyond CLI-specified JSON files and further integration with [`rule_coherence_checker.py`](simulation_engine/rules/rule_coherence_checker.py) are needed.",
    "issues": []
  },
  "simulation_engine/rules/rule_matching_utils.py": {
    "description": "Description: This module provides centralized utility functions for matching (exact, fuzzy), validating (schema validation via `rule_coherence_checker`), and accessing rule fingerprints from a `RuleRegistry` within the Pulse simulation engine. Main Gap: Key areas for improvement include implementing more sophisticated partial matching algorithms beyond the current common-key approach, enhancing error handling, and potentially optimizing performance for a very large number of rules (e.g., via indexing). Dedicated unit tests are also needed.",
    "issues": []
  },
  "simulation_engine/rules/rule_param_registry.py": {
    "description": "Defines tunable parameters for simulation rules.",
    "issues": []
  },
  "simulation_engine/rules/rule_registry.py": {
    "description": "Manages loading, storage, validation, and access to all rule types.",
    "issues": []
  },
  "simulation_engine/rules/static_rules.py": {
    "description": "Defines a basic registry of static causal rules for the Pulse simulation.",
    "issues": []
  },
  "simulation_engine/services/simulation_command.py": {
    "description": "Defines a command pattern for executing simulation operations.",
    "issues": []
  },
  "simulation_engine/services/simulation_runner.py": {
    "description": "Description: Orchestrates a sequence of simulation steps using the Command Pattern. A `SimulationRunner` class executes a list of command objects sequentially on a state object. Main Gap: Lacks error handling, logging, and advanced command features (undo/redo, conditional/parallel execution). State typing is `Any`, command interface is implicit.",
    "issues": []
  },
  "simulation_engine/utils/build_timeline.py": {
    "description": "Consolidates JSON snapshots into a single timeline file.",
    "issues": []
  },
  "simulation_engine/utils/ingest_to_snapshots.py": {
    "description": "Processes historical signals into per-turn WorldState snapshots.",
    "issues": []
  },
  "simulation_engine/utils/pulse_variable_forecaster.py": {
    "description": "Description: Forecasts future value trajectories of a selected simulation variable using Monte Carlo rollouts. Supports visualization and data export. Main Gap: No mechanism to start simulations from a specific pre-existing world state. Handling of non-numeric variables for averaging/plotting is unclear/unsupported. Lacks advanced statistical output beyond mean.",
    "issues": []
  },
  "simulation_engine/utils/simulation_replayer.py": {
    "description": "Description: Replays previously saved `WorldState` snapshots for audit, diagnostics, and retrodiction. Corrected attribute access for `WorldState.variables` and `WorldState.overlays`. Main Gap: The `show_lineage()` function for visualizing forecast ancestry is a stub. Detailed comparison for retrodiction outcomes is not explicitly implemented within the replay method.",
    "issues": []
  },
  "simulation_engine/utils/simulation_trace_logger.py": {
    "description": "Description: Provides utility functions for logging simulation traces to timestamped `.jsonl` files. Main Gap: Lacks explicit error handling for file I/O. Default log directory and aspects of file naming are hardcoded. Basic logging features; could be extended (rotation, levels, formats).",
    "issues": []
  },
  "simulation_engine/utils/simulation_trace_viewer.py": {
    "description": "Description: Provides a command-line utility for loading, inspecting, visualizing, and summarizing simulation trace files (`.jsonl`). Main Gap: Plotting capabilities could be extended. Summary is basic; lacks advanced analytical features. Default overlay keys for plotting are hardcoded.",
    "issues": []
  },
  "simulation_engine/utils/worldstate_io.py": {
    "description": "Description: Handles the persistence of WorldState objects, providing functionalities to save them to disk as JSON files and load them back into memory. This is crucial for simulation logging, replay, and audit trails. Main Gap: Potential enhancements include alternative serialization formats, schema versioning, batch operations, and alternative storage backends.",
    "issues": []
  },
  "simulation_engine/worldstate.py": {
    "description": "WorldState Module",
    "issues": []
  },
  "simulation_engine/variables/worldstate_variables.py": {
    "description": "Description: Defines the `WorldstateVariables` class to store, manage, and manipulate key simulation variables, supporting dictionary-like and attribute-style access, and methods for updates and decay. Main Gap: The decay rate and floor in `decay_variable()` are hardcoded; could be made configurable or support complex models.",
    "issues": []
  },
  "symbolic_system/gravity/cli.py": {
    "description": "Description: Provides a CLI for interacting with the Symbolic Gravity system, including enabling/disabling, configuring parameters (strength, learning rate), and printing status. Main Gap: Not standalone executable; status output is direct print; no config persistence.",
    "issues": []
  },
  "symbolic_system/gravity/engines/residual_gravity_engine.py": {
    "description": "Implements the Residual Gravity Engine, learning corrections to nudge simulations towards reality.",
    "issues": []
  },
  "symbolic_system/gravity/gravity_config.py": {
    "description": "Description: Manages configuration for the Symbolic Gravity Fabric system, including parameters for the residual gravity engine and symbolic pillar system. Centralizes defaults and allows overrides via kwargs, JSON files, and environment variables. Main Gap: Self-contained; primary next step is utilization by other gravity components.",
    "issues": []
  },
  "symbolic_system/gravity/gravity_fabric.py": {
    "description": "Description: Implements the core Symbolic Gravity Fabric, using symbolic pillars to dynamically adjust a residual gravity field, correcting simulation outputs to align with observed reality by learning from residuals. Main Gap: `ResidualGravityEngine` initialized with placeholder dt/dimensionality; hardcoded health suggestion thresholds; missing integration tests for the fabric itself.",
    "issues": []
  },
  "symbolic_system/gravity/integration.py": {
    "description": "This module integration layer for Symbolic Gravity, bridging overlays and pillars; lacks dedicated tests.",
    "issues": []
  },
  "symbolic_system/gravity/integration_example.py": {
    "description": "Demonstrates Symbolic Gravity integration in Pulse simulations and standalone.",
    "issues": []
  },
  "symbolic_system/gravity/overlay_bridge.py": {
    "description": "Provides compatibility between legacy overlays and new Symbolic Pillar/Gravity Fabric.",
    "issues": []
  },
  "symbolic_system/gravity/symbolic_gravity_fabric.py": {
    "description": "Implements the core Symbolic Gravity Fabric, a dynamic corrective layer using symbolic pillars.",
    "issues": []
  },
  "symbolic_system/gravity/symbolic_pillars.py": {
    "description": "Description: Implements \"Symbolic Pillars,\" dynamic data structures representing abstract symbolic concepts (e.g., Hope, Despair), forming the basis of the \"Symbolic Gravity Fabric.\" Main Gap: Advanced decay models, dynamic basis functions, and dynamic interaction matrix learning are potential future enhancements.",
    "issues": []
  },
  "symbolic_system/gravity/visualization.py": {
    "description": "Description: Provides utilities for visualizing the \"Symbolic Gravity Fabric\" system, offering terminal-based ASCII art and graphical plots using Matplotlib. Main Gap: The module fulfills its stated purpose; potential future extensions include more sophisticated/interactive visualization types and configurable styling.",
    "issues": []
  },
  "symbolic_system/config.py": {
    "description": "Description: The module provides enhanced and centralized configuration management for the symbolic system, supporting loading/saving from JSON, profile-based configurations for different market regimes, and flexible setting/retrieval of nested configuration values. It also includes functionality to detect market regimes and switch profiles. Major Gap: The detect_market_regime() method is explicitly described as \"simple\" and that it \"can be expanded with more sophisticated detection logic.\"",
    "issues": []
  },
  "symbolic_system/context.py": {
    "description": "Description: The module provides context management utilities for symbolic system processing, offering mechanisms to temporarily set the symbolic processing mode (e.g., \"simulation\", \"retrodiction\") and to check whether symbolic processing is enabled within the current operational context. Major Gap: The module is tightly coupled with global variables in core/pulse_config.py. While functional, a more robust system might involve a dedicated configuration object or service rather than direct manipulation of another module's globals.",
    "issues": []
  },
  "symbolic_system/numeric_transforms.py": {
    "description": "Description: The module provides bidirectional transformations between numeric indicators (statistical data) and symbolic overlay states, bridging quantitative data and qualitative symbolic representations. Major Gap: Extensibility of Mappings: While the module has default mappings and can load variable-specific ones, the mechanism for defining and managing a large, evolving set of these mappings isn't fully detailed.",
    "issues": []
  },
  "symbolic_system/optimization.py": {
    "description": "Description: The module provides performance optimization utilities for the symbolic system, including caching, lazy evaluation, and training-specific optimizations. Major Gap: Training-Specific Implementation Details: The `training_optimized` decorator allows for alternative implementations during training, but the module itself doesn't define these; they would reside in the modules using the decorator.",
    "issues": []
  },
  "symbolic_system/overlays.py": {
    "description": "Description: The module defines and manages symbolic overlay system (latent emotional-sentiment variables like \"Hope,\" \"Despair\"), providing functions to access, normalize, and modulate these overlay values and their interactions. Major Gap: Advanced Interactions: The current overlay interactions are relatively simple. More complex, nuanced, or conditional interactions could be developed, potentially driven by external configuration rather than being hardcoded.",
    "issues": []
  },
  "symbolic_system/pulse_symbolic_arc_tracker.py": {
    "description": "Description: Tracks, analyzes, and visualizes the distribution and changes of \"symbolic arcs\" within batches of forecasts. Core responsibilities include counting arc frequency, comparing distributions for \"drift,\" calculating stability, exporting summaries, and generating plots. Major Gap: Configurable Arc Labeling: The `compute_arc_label()` function uses hardcoded keywords and arc labels; this should be made flexible.",
    "issues": []
  },
  "symbolic_system/pulse_symbolic_learning_loop.py": {
    "description": "Learns symbolic strategy preferences from revision logs, tracking arc/tag performance.",
    "issues": []
  },
  "symbolic_system/pulse_symbolic_revision_planner.py": {
    "description": "Description: Suggests symbolic tuning recommendations for forecasts flagged as unstable or exhibiting drift, aiming to provide actionable plans for revising symbolic elements like overlay profiles, arc labels, and symbolic tags. Major Gap: Limited Scope of Revisions: The revision logic in `plan_symbolic_revision()` is based on a few specific hardcoded conditions. This could be expanded to handle a wider variety of scenarios or use a more dynamic/configurable rule set.",
    "issues": []
  },
  "symbolic_system/symbolic_alignment_engine.py": {
    "description": "Description: Compares symbolic tags (qualitative descriptors) with simulation variables (quantitative data) to determine an alignment score, quantifying how well a symbolic concept aligns with the current state of system variables. Major Gap: More Sophisticated Rules: The most significant gap is the lack of a comprehensive rule set or a more dynamic way to define and manage alignment rules. The current hardcoded `if/else` structure is not scalable.",
    "issues": []
  },
  "symbolic_system/symbolic_bias_tracker.py": {
    "description": "Tracks symbolic tag frequencies for bias analysis, offering CSV export and plotting.",
    "issues": []
  },
  "symbolic_system/symbolic_contradiction_cluster.py": {
    "description": "This module identifies and groups forecasts with symbolic contradictions based on overlay divergence and arc opposition.",
    "issues": []
  },
  "symbolic_system/symbolic_convergence_detector.py": {
    "description": "Description: The module measures, analyzes, and visualizes the convergence of symbolic labels within forecasts, quantifying narrative dominance and detecting fragmentation. Major Gap: The fragmentation threshold in `detect_fragmentation` is hardcoded (`0.4`) and could be made configurable. (Also, test coverage is likely missing).",
    "issues": []
  },
  "symbolic_system/symbolic_drift.py": {
    "description": "Description: The module detects and logs \"symbolic drift\" (rapid, unexpected, or contradictory changes in symbolic states/overlays) by comparing symbolic overlay values between consecutive WorldState objects. Major Gap: The module currently only detects and logs drift; logical next steps could involve implementing reactive measures. Default thresholds for tension and delta are hardcoded.",
    "issues": []
  },
  "symbolic_system/symbolic_executor.py": {
    "description": "This module applies symbolic upgrade plans to forecasts, rewrites overlays, and tracks transformations.",
    "issues": []
  },
  "symbolic_system/symbolic_flip_classifier.py": {
    "description": "Description: The module analyzes sequences of forecast \"chains\" to identify and classify transitions in symbolic \"arcs\" and \"tags,\" detecting common patterns and cyclical behaviors. Major Gap: The module's docstring mentions identifying \"Repair-resistant patterns for operator review,\" but there isn't an explicit function for this. Current loop detection is simple (A->B->A) and doesn't handle more complex cycles.",
    "issues": []
  },
  "symbolic_system/symbolic_memory.py": {
    "description": "Description: The module tracks and logs symbolic overlay states across different turns within each simulation for downstream symbolic analysis. Major Gap: The clear next step implied by this module is the development or integration of other modules that would consume and analyze the generated `jsonl` log files.",
    "issues": []
  },
  "symbolic_system/symbolic_state_tagger.py": {
    "description": "Description: The module interprets symbolic overlays (hope, despair, rage, fatigue) and assigns a descriptive symbolic tag to represent the emotional state of a simulation. Major Gap: The decision rules for tagging are hardcoded in an `if/elif/else` structure. A more data-driven or configurable rule system might be a logical next step for scalability.",
    "issues": []
  },
  "symbolic_system/symbolic_trace_scorer.py": {
    "description": "Description: The module scores symbolic trace histories based on narrative coherence, symbolic instability (volatility), and emotional arc structure. Major Gap: The current arc detection logic is based on simple trend direction and threshold checks. This could be expanded to recognize more nuanced or complex arc patterns.",
    "issues": []
  },
  "symbolic_system/symbolic_transition_graph.py": {
    "description": "Description: Constructs and visualizes a symbolic transition graph from forecast data, representing symbolic states as nodes and transitions as edges, with edge weights indicating frequency. Main Gap: Extensibility for different input data formats, advanced graph analytics, and interactive visualization are potential future enhancements.",
    "issues": []
  },
  "symbolic_system/symbolic_upgrade_planner.py": {
    "description": "Analyzes symbolic learning profiles to propose upgrade plans for arcs/tags.",
    "issues": []
  },
  "symbolic_system/symbolic_utils.py": {
    "description": "Description: Provides utility functions for working with symbolic overlays, including retrieval, normalization, and calculation of tension, fragility, and drift penalty scores. Main Gap: Extensibility for new symbolic overlays, advanced fragility metrics, and dynamic penalty calculation are potential future enhancements. Hardcoded overlay keys and coefficients.",
    "issues": []
  },
  "tests/conftest.py": {
    "description": "This module skipped - Skipped - Test File (per user request).",
    "issues": []
  },
  "trust_system/alignment_index.py": {
    "description": "Description: Calculates a \"Forecast Alignment Index\" (FAI), synthesizing confidence, retrodiction score, arc stability, symbolic tag match, and novelty into a single normalized score (0-100). Main Gap: The `trusted_tags` set and default component weights are hardcoded. Novelty calculation is basic.",
    "issues": []
  },
  "trust_system/forecast_audit_trail.py": {
    "description": "Description: Generates and logs audit records for forecasts, capturing performance metrics and metadata. Main Gap: Error handling for [`compute_retrodiction_error()`](trust_system/forecast_audit_trail.py:60) is generic and could be more specific.",
    "issues": []
  },
  "trust_system/forecast_episode_logger.py": {
    "description": "Description: Logs symbolic episode metadata associated with forecasts, including arc labels, symbolic tags, overlay states, confidence levels, and timestamps. It also provides utilities to read, summarize, and visualize this logged data. Main Gap: Advanced visualization capabilities beyond basic arc distribution plots are missing.",
    "issues": []
  },
  "trust_system/forecast_licensing_shell.py": {
    "description": "Description: Determines the eligibility of a given forecast for actions such as memory retention, export, or gaining operator trust by evaluating it against predefined criteria. Main Gap: Thresholds for confidence and alignment are managed via default dictionary values or optional parameters; integration with a formal project-wide configuration management system would improve maintainability.",
    "issues": []
  },
  "trust_system/forecast_memory_evolver.py": {
    "description": "Description: Analyzes historical regret data and forecast memory to adapt and improve the Pulse system's trust mechanisms, specifically by adjusting trust weights associated with rules based on their performance and identifying frequently problematic forecast traces. Main Gap: The trust weight adjustment is a fixed decrement; it could be made more adaptive, potentially based on the magnitude or impact of the regret.",
    "issues": []
  },
  "trust_system/fragility_detector.py": {
    "description": "Description: Calculates a \"fragility score\" for forecasts derived from symbolic tension and overlay volatility, aiming to quantify the emotional stability of a forecast. Main Gap: The thresholds for fragility labels are hardcoded and could be made configurable.",
    "issues": []
  },
  "trust_system/license_enforcer.py": {
    "description": "Description: Enforces trust-based licensing on forecasts, annotating them with status/explanation, filtering, and generating audit trails. Main Gap: The system/process for \"future learning\" from rejected forecasts is not detailed.",
    "issues": []
  },
  "trust_system/license_explainer.py": {
    "description": "Description: Provides human-readable explanations for forecast licensing status based on confidence, alignment, trust labels, and drift. Main Gap: Extensibility for new licensing criteria/rationale fields is the main area for future work.",
    "issues": []
  },
  "trust_system/pulse_lineage_tracker.py": {
    "description": "Description: Analyzes forecast history, tracing symbolic arc and rule logic changes across generations to understand stability and evolution. Main Gap: Current analysis is largely quantitative; could be expanded to include qualitative analysis, advanced error handling, scalability improvements, and visualization.",
    "issues": []
  },
  "trust_system/pulse_regret_chain.py": {
    "description": "This module manages a persistent log of \"regret events\" within the trust system, such as forecast errors or symbolic contradictions. It provides functionalities to record new events, retrieve the entire chain of regrets, update the review status of events, and generate summaries and scores based on the logged data. A command-line interface allows for direct interaction with the regret chain.",
    "issues": []
  },
  "trust_system/recovered_forecast_scorer.py": {
    "description": "This module re-evaluates forecasts that have been \"recovered,\" likely through a process like symbolic sweep or other corrective measures. It annotates these forecasts with updated trust and license metadata, flags those that remain unstable based on criteria like low alignment or drift, summarizes the quality of the repair process, and allows for exporting flagged forecasts for further review or revision.",
    "issues": []
  },
  "trust_system/retrodiction_engine.py": {
    "description": "This module serves as a wrapper for running retrodiction simulations. It utilizes the unified \\`simulate_forward\\` function from \\`engine.simulator_core\\` by setting it to \"retrodiction mode.\" The module handles the results of these simulations, providing functions to save them to a persistent \\`ForecastMemory\\` instance, ensuring that complex objects like overlays are properly serialized. It also includes a utility to save individual forecast objects.",
    "issues": []
  },
  "trust_system/rule_adjustment.py": {
    "description": "This module is responsible for dynamically adjusting the trust weights of rules and symbolic tags based on a \"learning profile.\" It analyzes performance metrics (like win rates) for symbolic arcs and tags, and then upgrades or downgrades their associated trust scores in the \\`RuleRegistry\\` and \\`VariableRegistry\\`. This allows the system to adapt its confidence in different rules and symbolic concepts over time based on their observed effectiveness.",
    "issues": []
  },
  "trust_system/services/trust_enrichment_service.py": {
    "description": "This module defines a service (\\`TrustEnrichmentService\\`) responsible for augmenting forecast objects with various trust-related metadata. It acts as an orchestrator, calling a series of private helper functions (imported from \\`trust_system.trust_engine\\`) to add information such as fragility scores, retrodiction errors, alignment scores, attention scores, regret linkage, and license status to a given forecast. It also supports a plugin architecture for custom enrichment steps.",
    "issues": []
  },
  "trust_system/services/trust_scoring_strategy.py": {
    "description": "This module defines a strategy pattern for calculating trust scores for forecasts. It includes an abstract base class `TrustScoringStrategy` and a `DefaultTrustScoringStrategy` that computes a forecast's confidence by considering factors like capital movement, risk, historical consistency, novelty, and symbolic drift.",
    "issues": []
  },
  "trust_system/symbolic_bandit_agent.py": {
    "description": "This module implements a simple Epsilon-Greedy multi-armed bandit agent. It's designed to select actions to maximize rewards over time by balancing exploration (random action) and exploitation (choosing the action with the current highest estimated value). The \\\"Symbolic\\\" part of its name suggests it's intended for use in contexts involving symbolic reasoning or choices, though the current implementation is a generic bandit agent.",
    "issues": []
  },
  "trust_system/trust_engine.py": {
    "description": "This module serves as the central engine for evaluating and managing trust in forecasts within the Pulse system. It integrates various sub-systems and functionalities, including symbolic tagging, confidence scoring (delegated to strategies like `DefaultTrustScoringStrategy`), risk assessment, contradiction detection, lineage tracking, and metadata enrichment. It provides a comprehensive suite of tools to assess, label, and audit forecasts, aiming to ensure their reliability and coherence for downstream decision-making processes. The engine is designed with extensibility in mind, featuring a plugin system for trust enrichment and a strategy pattern for scoring.",
    "issues": []
  },
  "trust_system/trust_update.py": {
    "description": "This module is responsible for adjusting the trust weights of historical forecasts stored in the \\\"PFPA (Past Forecast Performance Archive) memory.\\\" It uses retrodiction scores (how well a past forecast would have predicted actual outcomes) to modify the original confidence of these forecasts. Forecasts with high retrodiction scores get their trust weights boosted, while those with low scores are penalized. The module also includes a plugin system for custom actions after weights are updated and a simple inline test.",
    "issues": []
  },
  "trust_system/upgrade_gatekeeper.py": {
    "description": "Description: Manages the approval process of epistemic/symbolic upgrades by loading, scoring (currently simplified), and then approving or quarantining them based on a trust threshold. Main Gap: The primary gap is the simplified `score_upgrade` method, which needs to be replaced or enhanced with a robust trust evaluation mechanism, likely by integrating the planned `evaluate_upgrade_trust_score` from the TrustEngine.",
    "issues": []
  },
  "dev_tools/triage/build_triage_report.py": {
    "description": "Generates a triage report by parsing modules from inventory, scanning for issues in code files, and outputting a structured JSON report.",
    "issues": [
      "TODO (Line 9): 4. Scanning files for keywords: TODO, FIXME, Hardcoded, NotImplementedError (File: dev_tools/triage/build_triage_report.py)",
      "FIXME (Line 9): 4. Scanning files for keywords: TODO, FIXME, Hardcoded, NotImplementedError (File: dev_tools/triage/build_triage_report.py)",
      "Hardcoded (Line 9): 4. Scanning files for keywords: TODO, FIXME, Hardcoded, NotImplementedError (File: dev_tools/triage/build_triage_report.py)",
      "NotImplementedError (Line 9): 4. Scanning files for keywords: TODO, FIXME, Hardcoded, NotImplementedError (File: dev_tools/triage/build_triage_report.py)",
      "TODO (Line 34): KEYWORDS = [\"TODO\", \"FIXME\", \"Hardcoded\", \"NotImplementedError\"] (File: dev_tools/triage/build_triage_report.py)",
      "FIXME (Line 34): KEYWORDS = [\"TODO\", \"FIXME\", \"Hardcoded\", \"NotImplementedError\"] (File: dev_tools/triage/build_triage_report.py)",
      "Hardcoded (Line 34): KEYWORDS = [\"TODO\", \"FIXME\", \"Hardcoded\", \"NotImplementedError\"] (File: dev_tools/triage/build_triage_report.py)",
      "NotImplementedError (Line 34): KEYWORDS = [\"TODO\", \"FIXME\", \"Hardcoded\", \"NotImplementedError\"] (File: dev_tools/triage/build_triage_report.py)",
      "TODO (Line 182): {'keyword': 'TODO', 'line': 10, (File: dev_tools/triage/build_triage_report.py)",
      "TODO (Line 183): 'excerpt': 'TODO: Implement this function'} (File: dev_tools/triage/build_triage_report.py)"
    ]
  },
  "utils/context7_client.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "utils/error_utils.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "utils/file_utils.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "utils/log_utils.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "utils/performance_utils.py": {
    "description": "Description: TO DO",
    "issues": []
  },
  "visualization/trust_metrics_visualizer.py": {
    "description": "Generates visualizations (plots) and an HTML dashboard for trust metrics and Bayesian statistics of rules and variables. Key Gap: Static HTML output; could be interactive.",
    "issues": []
  }
}